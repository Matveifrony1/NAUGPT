# –û—Ç–∫–ª—é—á–∞–µ–º —Ç–µ–ª–µ–º–µ—Ç—Ä–∏—é ChromaDB
import os
os.environ["ANONYMIZED_TELEMETRY"] = "False"

import chromadb
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Optional
import uuid
import re
from datetime import datetime
from rapidfuzz import fuzz
from logger import get_logger

logger = get_logger(__name__)

class VectorDatabase:
    """–í–µ–∫—Ç–æ—Ä–Ω–∞ –±–∞–∑–∞ –¥–∞–Ω–∏—Ö –∑ –≥—ñ–±—Ä–∏–¥–Ω–∏–º –ø–æ—à—É–∫–æ–º (embedding + keyword)"""
    
    def __init__(self, db_path: str = "./nau_vector_db"):
        self.db_path = db_path
        self.collection_name = "nau_knowledge_base"
        self.embedding_model = None
        self.client = None
        self.collection = None
        self.similarity_threshold = 1.0
        
        self._initialize()
    
    def _initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ Jina Embeddings v3"""
        logger.debug("üöÄ –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤–µ–∫—Ç–æ—Ä–Ω–æ—ó –±–∞–∑–∏ –¥–∞–Ω–∏—Ö...")
        
        try:
            self.client = chromadb.PersistentClient(path=self.db_path)
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={
                    "description": "NAU News Knowledge Base",
                    "hnsw:space": "cosine"
                }
            )
            logger.info(f"‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞: {self.db_path}")
        except Exception as e:
            logger.critical(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —ñ–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—ó –ë–î: {e}")
            raise
        
        try:
            logger.debug("üì¶ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è Jina Embeddings v3...")
            
            import torch
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            logger.debug(f"üîß –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –ø—Ä–∏—Å—Ç—Ä—ñ–π: {device}")
            if torch.cuda.is_available():
                logger.debug(f"   GPU: {torch.cuda.get_device_name(0)}")
                logger.debug(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
            
            self.embedding_model = SentenceTransformer(
                'jinaai/jina-embeddings-v3',
                trust_remote_code=True,
                device=device  # ‚úÖ –¢–£–¢
            )
            logger.info(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –Ω–∞ {device}: jinaai/jina-embeddings-v3")
        except Exception as e:
            logger.critical(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
            raise
    
    def search_improved(self, query: str, top_k: int = 5, 
                       category_filter: Optional[str] = None) -> List[Dict]:
        """–ì–Ü–ë–†–ò–î–ù–ò–ô –ø–æ—à—É–∫: embedding + keyword search"""
        
        logger.debug(f"üîç –ì–Ü–ë–†–ò–î–ù–ò–ô –ü–û–®–£–ö: query='{query}', top_k={top_k}")
        
        if not self.embedding_model or not self.collection:
            return []
        
        # 1. Embedding –ø–æ–∏—Å–∫
        embedding_results = self._embedding_search(query, top_k * 10, category_filter)
        logger.debug(f"üîç EMBEDDING: {len(embedding_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        
        # 2. Keyword –ø–æ–∏—Å–∫
        keyword_results = self._keyword_search(query, top_k * 5, category_filter)
        logger.debug(f"üîç KEYWORD: {len(keyword_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        
        # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        combined_results = self._combine_results(embedding_results, keyword_results, top_k)
        logger.debug(f"üîç –ö–û–ú–ë–Ü–ù–û–í–ê–ù–ò–ô: {len(combined_results)} –æ—Å—Ç–∞—Ç–æ—á–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        
        return combined_results

    def search_with_route(self, query: str, route_decision, top_k: int = 5) -> List[Dict]:
        """
        –ü–æ—à—É–∫ –ë–ï–ó –¥—É–±–ª—é–≤–∞–Ω–Ω—è keywords
        """
        logger.debug(f"üîç SEARCH WITH ROUTE: scope={route_decision.search_scope}, entity={route_decision.target_entity}, top_k={top_k}")
        
        # 1. –ë—É–¥—É—î–º–æ —Ñ—ñ–ª—å—Ç—Ä–∏
        where_filter = self._build_route_filters(route_decision)
        
        # 2. –î–æ–¥–∞—î–º–æ keywords –¢–Ü–õ–¨–ö–ò —è–∫—â–æ —ó—Ö –Ω–µ–º–∞—î –≤ query
        enhanced_query = query  # –ü–æ—á–∏–Ω–∞—î–º–æ –∑ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–≥–æ query (—è–∫–∏–π –≤–∂–µ –º–∞—î keywords –∑ assistant.py)
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —è–∫—ñ keywords —â–µ –Ω–µ –≤ –∑–∞–ø–∏—Ç—ñ
        if route_decision.enhancement_keywords:
            query_words_lower = set(query.lower().split())
            new_keywords = [
                kw for kw in route_decision.enhancement_keywords 
                if kw.lower() not in query_words_lower
            ]
            
            # –î–æ–¥–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ù–û–í–Ü keywords
            if new_keywords:
                enhanced_query = query + " " + " ".join(new_keywords)
                logger.debug(f"üîç –î–û–î–ê–ù–Ü –ù–û–í–Ü KEYWORDS: {new_keywords}")
            else:
                logger.debug(f"üîç –í–°–Ü KEYWORDS –í–ñ–ï –Ñ –í –ó–ê–ü–ò–¢–Ü")
        
        logger.debug(f"üîç ENHANCED QUERY: '{enhanced_query}'")
        
        # 3. –í–∏–∫–æ–Ω—É—î–º–æ –ø–æ—à—É–∫ (–∑–±—ñ–ª—å—à—É—î–º–æ –º–Ω–æ–∂–Ω–∏–∫ –¥–ª—è top_k=6)
        results = self._route_aware_search(enhanced_query, where_filter, top_k * 5)
        
        # 4. –ü–µ—Ä–µ—Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è –∑ –±–æ–Ω—É—Å–∞–º–∏
        ranked = self._rerank_with_route_bonuses(results, route_decision)
        
        # 5. –ü–æ–≤–µ—Ä—Ç–∞—î–º–æ —Ç–æ–ø-K
        final_results = ranked[:top_k]
        
        logger.debug(f"üîç –ü–û–í–ï–†–ù–£–¢–û: {len(final_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        for i, r in enumerate(final_results, 1):
            title = r['metadata'].get('title', 'No title')
            score = r.get('relevance_score', 0)
            logger.debug(f"  #{i}: score={score:.1f}, title='{title}...'")
        
        return final_results


    def _build_route_filters(self, route_decision) -> Dict:
        """–°—Ç–≤–æ—Ä–µ–Ω–Ω—è ChromaDB —Ñ—ñ–ª—å—Ç—Ä—ñ–≤ –∑ RouteDecision"""
        conditions = [{"source": {"$eq": "news"}}]
        
        # –§—ñ–ª—å—Ç—Ä –ø–æ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç—É
        if route_decision.search_scope != "global":
            conditions.append({"faculty": {"$eq": route_decision.search_scope}})
        
        # –§—ñ–ª—å—Ç—Ä –ø–æ –∫–∞—Ñ–µ–¥—Ä—ñ
        if route_decision.target_entity:
            conditions.append({"department": {"$eq": route_decision.target_entity}})
        
        # –§—ñ–ª—å—Ç—Ä –ø–æ intent (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è)
        intent_to_category = {
            "schedule": "schedule",
            "news": None,  # –ù–µ —Ñ—ñ–ª—å—Ç—Ä—É—î–º–æ
            "events": "conferences",
            "contacts": "contacts",
            "info": None
        }
        
        category = intent_to_category.get(route_decision.search_intent)
        if category:
            conditions.append({"category": {"$eq": category}})
        
        return {"$and": conditions} if len(conditions) > 1 else conditions[0]


    def _route_aware_search(self, query: str, where_filter: Dict, max_results: int) -> List[Dict]:
        """–ü–æ—à—É–∫ –∑ route —Ñ—ñ–ª—å—Ç—Ä–∞–º–∏ + Jina"""
        try:
            prefixed_query = f"query: {query}"
            
            query_embedding = self.embedding_model.encode(
                [prefixed_query],
                task='retrieval.query',
                prompt_name='retrieval.query'
            )[0].tolist()
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=max_results,
                where=where_filter
            )
            
            formatted = []
            if results["documents"] and results["documents"][0]:
                for i in range(len(results["documents"][0])):
                    formatted.append({
                        "content": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "distance": results["distances"][0][i] if results["distances"] else 1.0,
                        "relevance_score": max(0, 1.0 - (results["distances"][0][i] / 4)),
                        "id": results["ids"][0][i] if results["ids"] else None,
                        "search_type": "routed"
                    })
            
            return formatted
        except Exception as e:
            logger.error(f"‚ùå Route search error: {e}")
            return []


    def _rerank_with_route_bonuses(self, results: List[Dict], route_decision) -> List[Dict]:
        """
        –ë–æ–Ω—É—Å–∏ –∑–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ñ –¥–æ–∫—É–º–µ–Ω—Ç–∏
        """
        for result in results:
            meta = result["metadata"]
            base_score = result["relevance_score"]
            bonus = 0
            
            if route_decision.search_scope != "global":
                if meta.get("faculty") == route_decision.search_scope:
                    bonus += 0.1
                
                if meta.get("department") == route_decision.target_entity:
                    bonus += 0.1
            
            result["relevance_score"] = base_score + bonus
            result["route_bonus"] = bonus
        
        results.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        # –õ–æ–≥—É–≤–∞–Ω–Ω—è —Ç–æ–ø-3
        for i, r in enumerate(results[:3]):
            logger.debug(f"  #{i+1}: score={r['relevance_score']:.3f} (bonus={r.get('route_bonus', 0):.3f})")
        
        return results


    def _embedding_search(self, query: str, max_results: int, category_filter: Optional[str]) -> List[Dict]:
        """–ü–æ—à—É–∫ —á–µ—Ä–µ–∑ Jina embedding"""
        try:
            expanded_query = self._expand_query_improved(query)
            logger.debug(f"üîç –†–û–ó–®–ò–†–ï–ù–ò–ô –ó–ê–ü–†–û–°: '{expanded_query}'")
            
            # Jina prefix –¥–ª—è –∑–∞–ø–∏—Ç—É
            prefixed_query = f"query: {expanded_query}"
            
            query_embedding = self.embedding_model.encode(
                [prefixed_query],
                task='retrieval.query',
                prompt_name='retrieval.query'
            )[0].tolist()
            
            logger.debug(f"üîç EMBEDDING: –°—Ç–≤–æ—Ä–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ {len(query_embedding)}")
            
            where_clause = {"source": "news"}
            if category_filter:
                where_clause = {
                    "$and": [
                        {"source": {"$eq": "news"}},
                        {"category": {"$eq": category_filter}}
                    ]
                }
            
            search_count = min(max_results * 10, 500)
            logger.debug(f"üîç EMBEDDING –ü–û–®–£–ö: –ó–∞–ø—Ä–∞—à—É—î–º–æ {search_count} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=search_count,
                where=where_clause
            )
            
            found_count = len(results["documents"][0]) if results["documents"] else 0
            logger.debug(f"üîç EMBEDDING –†–ï–ó–£–õ–¨–¢–ê–¢–ò: –û—Ç—Ä–∏–º–∞–Ω–æ {found_count} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
            
            formatted_results = []
            if results["documents"] and len(results["documents"][0]) > 0:
                for i in range(len(results["documents"][0])):
                    distance = results["distances"][0][i] if results["distances"] else 1.0
                    metadata = results["metadatas"][0][i] if results["metadatas"] else {}
                    
                    formatted_results.append({
                        "content": results["documents"][0][i],
                        "metadata": metadata,
                        "distance": distance,
                        "relevance_score": max(0, 1.0 - (distance / 4)),
                        "id": results["ids"][0][i] if results["ids"] else None,
                        "search_type": "embedding"
                    })
                    
                    if i < 5:
                        title = metadata.get('title', 'No title')[:40]
                        logger.debug(f"üîç EMBEDDING #{i+1}: distance={distance:.3f}, title='{title}...'")
            
            formatted_results.sort(key=lambda x: x["distance"])
            return formatted_results[:max_results]
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ embedding –ø–æ—à—É–∫—É: {e}")
            return []
    
    def _keyword_search(self, query: str, max_results: int, category_filter: Optional[str]) -> List[Dict]:
        """–ü–æ—à—É–∫ –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏ –≤ –∑–º—ñ—Å—Ç—ñ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤"""
        try:
            logger.debug(f"üîç KEYWORD –ü–û–ò–°–ö: '{query}'")
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
            keywords = self._extract_keywords(query)
            logger.debug(f"üîç –ö–õ–Æ–ß–û–í–Ü –°–õ–û–í–ê: {keywords}")
            
            # –û—Ç—Ä–∏–º—É—î–º–æ –í–°–Ü –¥–æ–∫—É–º–µ–Ω—Ç–∏ –¥–ª—è –ø–æ—à—É–∫—É –∑–∞ –∑–º—ñ—Å—Ç–æ–º –∑ –ø—Ä–∞–≤–∏–ª—å–Ω–∏–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–æ–º
            if category_filter:
                where_clause = {
                    "$and": [
                        {"source": {"$eq": "news"}},
                        {"category": {"$eq": category_filter}}
                    ]
                }
            else:
                where_clause = {"source": {"$eq": "news"}}
            
            all_docs = self.collection.get(where=where_clause)
            if not all_docs["documents"]:
                return []
            
            logger.debug(f"üîç KEYWORD: –ê–Ω–∞–ª—ñ–∑—É—î–º–æ {len(all_docs['documents'])} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
            
            scored_results = []
            for i, doc_content in enumerate(all_docs["documents"]):
                metadata = all_docs["metadatas"][i] if all_docs["metadatas"] else {}
                doc_id = all_docs["ids"][i] if all_docs["ids"] else str(i)
                
                # –ü—ñ–¥—Ä–∞—Ö–æ–≤—É—î–º–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å –∑–∞ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
                score = self._calculate_keyword_score(doc_content, keywords, metadata)
                
                if score > 0:
                    scored_results.append({
                        "content": doc_content,
                        "metadata": metadata,
                        "distance": 1.0 - (score / 10),  # –ö–æ–Ω–≤–µ—Ä—Ç—É—î–º–æ score –≤ distance
                        "relevance_score": score,
                        "id": doc_id,
                        "search_type": "keyword"
                    })
            
            # –°–æ—Ä—Ç—É—î–º–æ –∑–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—é
            scored_results.sort(key=lambda x: x["relevance_score"], reverse=True)
            
            # –õ–æ–≥—É—î–º–æ —Ç–æ–ø-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
            for i, result in enumerate(scored_results[:5]):
                title = result["metadata"].get('title', 'No title')[:40]
                score = result["relevance_score"]
                logger.debug(f"üîç KEYWORD #{i+1}: score={score:.1f}, title='{title}...'")
            
            return scored_results[:max_results]
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ keyword –ø–æ—à—É–∫—É: {e}")
            return []
    
    def _extract_keywords(self, query: str) -> List[str]:
        stop_words = {
            '—è–∫', '–º–µ–Ω—ñ', '–¥–ª—è', '–Ω–∞', '–≤', '–∑', '–ø–æ', '—ñ', '–∞', '–∞–ª–µ', '–∞–±–æ', '—Ç–µ', '—â–æ',
            '—Ö—Ç–æ', '–∫–æ–ª–∏', '–¥–µ', '—á–æ–º—É', '—è–∫–∏–π', '—è–∫–∞', '—è–∫–µ', '—á–∏', '—î', '–±—É–≤', '–±—É–ª–∞',
            '—Ä–æ–∑–∫–∞–∂–∏', '—Å–∫–∞–∂–∏', '–ø–æ–∫–∞–∂–∏', '–∑–Ω–∞–π–¥–∏', '–¥–∞–π', '—Ç–∞–∫–∏–π', '—Ç–∞–∫–∞', '—Ç–∞–∫–µ'
        }
        
        clean_query = re.sub(r'[^\w\s]', ' ', query.lower())
        words = [w.strip() for w in clean_query.split() if len(w.strip()) > 2]
        keywords = [w for w in words if w not in stop_words]
        
        # –î–æ–¥–∞—î–º–æ –ø–æ–≤–Ω–∏–π –∑–∞–ø–∏—Ç —è–∫—â–æ —Ç–∞–º 1-2 –∑–Ω–∞—á—É—â–∏—Ö —Å–ª–æ–≤–∞
        if len(keywords) <= 2 and keywords:
            keywords.append(' '.join(keywords))
        
        return keywords
    
    def _calculate_keyword_score(self, content: str, keywords: List[str], metadata: Dict) -> float:
        content_lower = content.lower()
        title = metadata.get('title', '').lower()
        
        score = 0.0
        
        for keyword in keywords:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫ - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç
            if keyword in title:
                score += 30.0
            
            # –¢–æ—á–Ω—ñ —Å–ø—ñ–≤–ø–∞–¥—ñ–Ω–Ω—è –≤ –ü–û–í–ù–û–ú–£ —Ç–µ–∫—Å—Ç—ñ
            content_count = content_lower.count(keyword)
            score += content_count * 3.0
            
            # Fuzzy –¥–ª—è —Å–ª—ñ–≤ 4+ —Å–∏–º–≤–æ–ª–∏
            if len(keyword) > 3:
                # –ó–∞–≥–æ–ª–æ–≤–æ–∫
                for word in title.split():
                    if fuzz.ratio(keyword, word) > 85:
                        score += 8.0
                        break
                
                # –í–µ—Å—å —Ç–µ–∫—Å—Ç (–æ–±–º–µ–∂—É—î–º–æ –ø–µ—Ä–µ–≤—ñ—Ä–∫—É –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ)
                found_fuzzy = False
                for word in content_lower.split():
                    if fuzz.ratio(keyword, word) > 85:
                        score += 4.0
                        found_fuzzy = True
                        break
        
        return score
    
    def _combine_results(self, embedding_results: List[Dict], keyword_results: List[Dict], top_k: int) -> List[Dict]:
        combined = {}
        
        # Embedding –∑ –≤–∏—Å–æ–∫–∏–º–∏ —Å–∫–æ—Ä–∞–º–∏
        for i, result in enumerate(embedding_results):
            doc_id = result["id"]
            result["embedding_rank"] = i + 1
            result["keyword_rank"] = None
            
            distance = result["distance"]
            base = max(0, (2.0 - distance) * 50)
            position_bonus = max(0, (10 - i) * 5)
            
            result["relevance_score"] = base + position_bonus
            result["search_type"] = "embedding"
            combined[doc_id] = result
        
        # Keyword —è–∫ –±—É—Å—Ç–µ—Ä –∞–±–æ –Ω–æ–≤—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        for i, result in enumerate(keyword_results):
            doc_id = result["id"]
            if doc_id in combined:
                # –ë—É—Å—Ç–∏–º–æ —è–∫—â–æ —î –≤ –æ–±–æ—Ö
                keyword_boost = result["relevance_score"] * 2
                combined[doc_id]["relevance_score"] += keyword_boost
                combined[doc_id]["search_type"] = "hybrid"
                combined[doc_id]["keyword_rank"] = i + 1
            else:
                # –î–æ–¥–∞—î–º–æ —è–∫ –Ω–æ–≤–∏–π
                result["keyword_rank"] = i + 1
                result["embedding_rank"] = None
                result["relevance_score"] = result["relevance_score"] * 3
                result["search_type"] = "keyword"
                combined[doc_id] = result
        
        final = list(combined.values())
        final.sort(key=lambda x: x["relevance_score"], reverse=True)
        
        return final[:top_k]
    
    def _expand_query_improved(self, query: str) -> str:
        """–ü–û–ö–†–ê–©–ï–ù–ï —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—É –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –ö–ê–Ü"""
        
        # –°–ª–æ–≤–Ω–∏–∫ —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤ —ñ –ø–æ–≤'—è–∑–∞–Ω–∏—Ö —Ç–µ—Ä–º—ñ–Ω—ñ–≤ –¥–ª—è –ö–ê–Ü
        synonyms = {
            # –§—ñ–Ω–∞–Ω—Å–∏ —Ç–∞ –æ–ø–ª–∞—Ç–∞
            "–æ–ø–ª–∞—Ç": ["–ø–ª–∞—Ç", "–ø–ª–∞—Ç—ñ–∂", "–ø–ª–∞—Ç–∏—Ç–∏", "–∑–∞–ø–ª–∞—Ç–∏—Ç–∏", "–≥—Ä–æ—à—ñ", "–∫–æ—à—Ç", "–≤–∞—Ä—Ç—ñ—Å—Ç—å", "—Ä–µ–∫–≤—ñ–∑–∏—Ç", "–±–∞–Ω–∫", "—Ä–∞—Ö—É–Ω–æ–∫", "–∫–≤–∏—Ç–∞–Ω—Ü", "–ø–µ—Ä–µ–∫–∞–∑"],
            "–ø–ª–∞—Ç": ["–æ–ø–ª–∞—Ç", "–ø–ª–∞—Ç—ñ–∂", "–ø–ª–∞—Ç–∏—Ç–∏", "–∑–∞–ø–ª–∞—Ç–∏—Ç–∏", "–≥—Ä–æ—à—ñ", "–∫–æ—à—Ç"],
            "—É—á—ë–±": ["–Ω–∞–≤—á–∞–Ω–Ω—è", "–æ—Å–≤—ñ—Ç", "—Å—Ç—É–¥–µ–Ω—Ç", "—Å–µ–º–µ—Å—Ç—Ä", "–∫—É—Ä—Å", "—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç", "–ù–ê–£"],
            "—É—á–µ–±": ["–Ω–∞–≤—á–∞–Ω–Ω—è", "–æ—Å–≤—ñ—Ç", "—Å—Ç—É–¥–µ–Ω—Ç", "—Å–µ–º–µ—Å—Ç—Ä", "–∫—É—Ä—Å"],
            "—Ä–µ–∫–≤—ñ–∑–∏—Ç": ["–±–∞–Ω–∫—ñ–≤—Å—å–∫", "—Ä–∞—Ö—É–Ω–æ–∫", "–∫–æ–¥", "iban", "–æ–ø–ª–∞—Ç", "–ø–µ—Ä–µ–∫–∞–∑", "–∫–≤–∏—Ç–∞–Ω—Ü"],
            "—Ä–µ–∫–≤–∏–∑–∏—Ç": ["—Ä–µ–∫–≤—ñ–∑–∏—Ç", "–±–∞–Ω–∫—ñ–≤—Å—å–∫", "—Ä–∞—Ö—É–Ω–æ–∫", "–∫–æ–¥", "–æ–ø–ª–∞—Ç"],
            
            # –ù–∞–≤—á–∞–Ω–Ω—è
            "–Ω–∞–≤—á–∞–Ω–Ω—è": ["–æ—Å–≤—ñ—Ç", "–ª–µ–∫—Ü—ñ", "—Å–µ–º—ñ–Ω–∞—Ä", "–∑–∞–Ω—è—Ç—Ç—è", "–∫—É—Ä—Å", "—Å—Ç—É–¥–µ–Ω—Ç"],
            "—Å—Ç—É–¥–µ–Ω—Ç": ["—É—á–Ω", "—Å–ª—É—Ö–∞—á", "–∫—É—Ä—Å–∞–Ω—Ç", "–≥—Ä—É–ø", "—Ñ–∞–∫—É–ª—å—Ç–µ—Ç"],
            "—É–Ω—ñ–≤–µ—Ä—Å–∏—Ç–µ—Ç": ["–ù–ê–£", "–≤–∏—à", "—ñ–Ω—Å—Ç–∏—Ç—É—Ç", "—Ñ–∞–∫—É–ª—å—Ç–µ—Ç", "–∫–∞—Ñ–µ–¥—Ä"],
            
            # –ù–∞—É–∫–∞
            "–Ω–∞—É–∫": ["–¥–æ—Å–ª—ñ–¥–∂", "–ø—É–±–ª—ñ–∫–∞—Ü", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü", "—Å–∏–º–ø–æ–∑—ñ—É–º", "–¥–∏—Å–µ—Ä—Ç–∞—Ü"],
            "—Å–ø—ñ–≤–ø—Ä–∞—Ü": ["–ø–∞—Ä—Ç–Ω–µ—Ä", "—É–≥–æ–¥", "–ø—Ä–æ–µ–∫—Ç", "—ñ–Ω—ñ—Ü—ñ–∞—Ç–∏–≤", "–¥–æ–≥–æ–≤—ñ—Ä"],
            
            # –°–æ–±—ã—Ç–∏—è
            "–Ω–æ–≤–∏–Ω": ["–ø–æ–¥—ñ–π", "–∑–∞—Ö–æ–¥", "—ñ–Ω—Ñ–æ—Ä–º–∞—Ü", "–ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω", "–æ–≥–æ–ª–æ—à–µ–Ω"],
            "–≤—Å—Ç—É–ø": ["–∞–±—ñ—Ç—É—Ä—ñ—î–Ω—Ç", "–ø—Ä–∏–π–æ–º", "–¥–æ–∫—É–º–µ–Ω—Ç", "–∫–æ–Ω–∫—É—Ä—Å", "–∑–∞—Ä–∞—Ö—É–≤–∞–Ω"],
            "—Ä–æ–∑–∫–ª–∞–¥": ["–ø–∞—Ä", "–∑–∞–Ω—è—Ç—Ç—è", "—á–∞—Å", "–≥—Ä–∞—Ñ—ñ–∫", "–ª–µ–∫—Ü—ñ"]
        }
        
        query_lower = query.lower()
        expanded_terms = [query]
        
        # –î–æ–¥–∞—î–º–æ —Å–∏–Ω–æ–Ω—ñ–º–∏ –¥–ª—è –∑–Ω–∞–π–¥–µ–Ω–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤
        for key, values in synonyms.items():
            if key in query_lower:
                expanded_terms.extend(values[:5])  # –ë–µ—Ä–µ–º–æ —Ç–æ–ø 5 —Å–∏–Ω–æ–Ω—ñ–º—ñ–≤
                logger.debug(f"üîç –°–ò–ù–û–ù–ò–ú–´ –¥–ª—è '{key}': {values[:5]}")
            
            # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ –∑–≤–æ—Ä–æ—Ç–Ω—ñ –∑–±—ñ–≥–∏
            for value in values:
                if value in query_lower:
                    expanded_terms.append(key)
                    expanded_terms.extend([v for v in values[:3] if v != value])
                    logger.debug(f"üîç –û–ë–†–ê–¢–ù–´–ï –°–ò–ù–û–ù–ò–ú–´ –¥–ª—è '{value}': {key}")
                    break
        
        # –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏ —ñ –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ
        unique_terms = list(set(expanded_terms))
        expanded_query = " ".join(unique_terms[:15])  # –ó–±—ñ–ª—å—à—É—î–º–æ –ª—ñ–º—ñ—Ç
        
        return expanded_query
    
    def add_documents(self, documents: List[Dict], batch_size: int = 1):
        """–î–æ–¥–∞–≤–∞–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ Jina embeddings"""
        if not documents:
            return
        
        logger.info(f"üìù –î–æ–¥–∞–≤–∞–Ω–Ω—è {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –≤ –±–∞–∑—É...")
        
        for i in range(0, len(documents), batch_size):
            batch = documents[i:i + batch_size]
            
            ids = []
            contents_full = []      # –ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç –¥–ª—è –ë–î
            contents_embedding = [] # –æ–±—Ä—ñ–∑–∞–Ω–∏–π –¥–ª—è embedding
            metadatas = []
            
            for doc in batch:
                doc_id = doc.get("id", str(uuid.uuid4()))
                ids.append(doc_id)
                
                content = doc.get("content", "")
                
                # –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –ü–û–í–ù–ò–ô –¢–ï–ö–°–¢
                contents_full.append(content)
                
                # –î–õ–Ø EMBEDDING - –û–ë–†–Ü–ó–ê–Ñ–ú–û –î–û 10000
                if len(content) > 4000:
                    content_short = content[:4000]
                else:
                    content_short = content
                contents_embedding.append(content_short)
                
                metadata = doc.get("metadata", {})
                if "added_at" not in metadata:
                    metadata["added_at"] = datetime.now().isoformat()
                metadatas.append(metadata)
            
            try:
                # EMBEDDING –ó –û–ë–†–Ü–ó–ê–ù–û–ì–û –¢–ï–ö–°–¢–£
                prefixed_contents = [f"passage: {text}" for text in contents_embedding]
                
                embeddings = self.embedding_model.encode(
                    prefixed_contents,
                    task='retrieval.passage',
                    prompt_name='retrieval.passage'
                ).tolist()
                
                # –ó–ë–ï–†–Ü–ì–ê–Ñ–ú–û –ü–û–í–ù–ò–ô –¢–ï–ö–°–¢ –í –ë–î
                self.collection.add(
                    ids=ids,
                    embeddings=embeddings,        # –ó –æ–±—Ä—ñ–∑–∞–Ω–æ–≥–æ
                    documents=contents_full,      # –ü–û–í–ù–ò–ô —Ç–µ–∫—Å—Ç!
                    metadatas=metadatas
                )
                
                logger.info(f"  ‚úì –î–æ–¥–∞–Ω–æ {len(batch)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ (–±–∞—Ç—á {i//batch_size + 1})")
                
            except Exception as e:
                logger.critical(f"  ‚úó –ü–æ–º–∏–ª–∫–∞ –¥–æ–¥–∞–≤–∞–Ω–Ω—è –±–∞—Ç—á—É: {e}")
    
    def search(self, query: str, top_k: int = 5, 
              category_filter: Optional[str] = None,
              source_filter: Optional[str] = None,
              metadata_filter: Optional[Dict] = None) -> List[Dict]:
        """–ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è—î–º–æ –Ω–∞ –≥—ñ–±—Ä–∏–¥–Ω–∏–π –ø–æ—à—É–∫ –∑ –æ–±—Ä–æ–±–∫–æ—é —Ñ—ñ–ª—å—Ç—Ä—ñ–≤"""
        # –ü–µ—Ä–µ–¥–∞—î–º–æ –≤—Å—ñ —Ñ—ñ–ª—å—Ç—Ä–∏ –≤ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–π –ø–æ—à—É–∫
        return self.search_improved(query, top_k, category_filter)
    
    def search_schedule(self, group: str, day: Optional[str] = None, 
                       week: Optional[int] = None) -> List[Dict]:
        """–°–ø–µ—Ü—ñ–∞–ª—ñ–∑–æ–≤–∞–Ω–∏–π –ø–æ—à—É–∫ –ø–æ —Ä–æ–∑–∫–ª–∞–¥—É + Jina"""
        logger.debug(f"üìÖ –ü–û–®–£–ö –†–û–ó–ö–õ–ê–î–£: group='{group}', day='{day}', week={week}")
        
        where_conditions = [
            {"source": {"$eq": "news"}},
            {"category": {"$eq": "schedule"}},
            {"group": {"$eq": group}}
        ]
        
        if day:
            where_conditions.append({"day": {"$eq": day}})
        if week:
            where_conditions.append({"week": {"$eq": str(week)}})
        
        where_clause = {"$and": where_conditions} if len(where_conditions) > 1 else where_conditions[0]
        
        try:
            query_text = f"query: —Ä–æ–∑–∫–ª–∞–¥ {group} {day if day else ''} {week if week else ''}"
            
            query_embedding = self.embedding_model.encode(
                [query_text],
                task='retrieval.query',
                prompt_name='retrieval.query'
            )[0].tolist()
            
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=20,
                where=where_clause
            )
            
            formatted_results = []
            if results["documents"] and len(results["documents"][0]) > 0:
                for i in range(len(results["documents"][0])):
                    formatted_results.append({
                        "content": results["documents"][0][i],
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "distance": results["distances"][0][i] if results["distances"] else 1.0,
                        "relevance_score": 1.0 - (results["distances"][0][i] if results["distances"] else 1.0),
                        "id": results["ids"][0][i] if results["ids"] else None,
                        "search_type": "schedule"
                    })
            
            logger.debug(f"üìÖ –ü–û–®–£–ö –†–û–ó–ö–õ–ê–î–£ –†–ï–ó–£–õ–¨–¢–ê–¢: –ó–Ω–∞–π–¥–µ–Ω–æ {len(formatted_results)} –∑–∞–ø–∏—Å—ñ–≤")
            return formatted_results
            
        except Exception as e:
            logger.error(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø–æ—à—É–∫—É —Ä–æ–∑–∫–ª–∞–¥—É: {e}")
            return []
    
    def search_recent_news(self, days: int = 7, category: Optional[str] = None) -> List[Dict]:
        """–ü–æ—à—É–∫ –æ—Å—Ç–∞–Ω–Ω—ñ—Ö –Ω–æ–≤–∏–Ω"""
        from datetime import datetime, timedelta
        cutoff_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        
        metadata_filter = {}
        if category:
            metadata_filter["category"] = category
        
        results = self.search(
            query="–Ω–æ–≤–∏–Ω–∏ –ù–ê–£ –æ—Å—Ç–∞–Ω–Ω—ñ –ø–æ–¥—ñ—ó",
            top_k=20,
            source_filter="news",
            metadata_filter=metadata_filter
        )
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∑–∞ –¥–∞—Ç–æ—é, —è–∫—â–æ —î
        filtered = []
        for result in results:
            news_date = result["metadata"].get("date", "")
            if news_date:
                try:
                    for fmt in ["%d/%m/%Y", "%d-%m-%Y", "%Y-%m-%d"]:
                        try:
                            parsed_date = datetime.strptime(news_date, fmt)
                            if parsed_date.strftime("%Y-%m-%d") >= cutoff_date:
                                filtered.append(result)
                            break
                        except:
                            continue
                except:
                    filtered.append(result)
            else:
                filtered.append(result)
        
        return filtered
    
    def debug_search_info(self, query: str) -> Dict:
        """–ù–∞–ª–∞–≥–æ–¥–∂—É–≤–∞–ª—å–Ω–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –ø–æ—à—É–∫"""
        logger.debug(f"–ê–Ω–∞–ª—ñ–∑ –∑–∞–ø–∏—Ç—É '{query}'")
        
        try:
            # –Ü–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –ø—Ä–æ –∫–æ–ª–µ–∫—Ü—ñ—é
            collection_count = self.collection.count()
            logger.debug(f"–î–æ–∫—É–º–µ–Ω—Ç—ñ–≤ —É –∫–æ–ª–µ–∫—Ü—ñ—ó: {collection_count}")
            
            # –ü—Ä–∏–∫–ª–∞–¥ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –∑ –ü–û–í–ù–ò–ú –∑–º—ñ—Å—Ç–æ–º
            sample = self.collection.get(limit=3)
            logger.debug(f"–ü—Ä–∏–∫–ª–∞–¥–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤:")
            if sample["documents"]:
                for i, doc in enumerate(sample["documents"][:3]):
                    metadata = sample["metadatas"][i] if sample["metadatas"] else {}
                    title = metadata.get("title", "No title")
                    content_preview = doc[:200] if doc else "No content"
                    print(f"  {i+1}: Title: {title}")
                    print(f"      Content: {content_preview}...")
                    print(f"      Length: {len(doc)} chars")
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫ keyword
            keywords = self._extract_keywords(query)
            logger.debug(f"–í–∏—Ç—è–≥–Ω—É—Ç—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞: {keywords}")
            
            # –¢–µ—Å—Ç–æ–≤–∏–π –ø–æ—à—É–∫ –∑ –¥—É–∂–µ –≤–∏—Å–æ–∫–∏–º top_k
            test_results = self.collection.query(
                query_embeddings=[self.embedding_model.encode([query]).tolist()[0]],
                n_results=20,
                where={"source": "news"}
            )
            
            if test_results["distances"]:
                distances = test_results["distances"][0]
                logger.debug(f"–¢–æ–ø –¥–∏—Å—Ç–∞–Ω—Ü—ñ—ó: {[f'{d:.3f}' for d in distances[:10]]}")
                logger.debug(f"–ú—ñ–Ω distance: {min(distances):.3f}")
                logger.debug(f"–ú–∞–∫—Å distance: {max(distances):.3f}")
                logger.debug(f"–°–µ—Ä–µ–¥–Ω—ñ–π distance: {sum(distances)/len(distances):.3f}")
                
                # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ, —Å–∫—ñ–ª—å–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –ø—Ä–æ—Ö–æ–¥—è—Ç—å —Ä—ñ–∑–Ω—ñ –ø–æ—Ä–æ–≥–∏
                for threshold in [0.5, 1.0, 1.5, 2.0, 2.5, 3.0]:
                    count = sum(1 for d in distances if d <= threshold)
                    logger.debug(f"–ü—Ä–∏ –ø–æ—Ä–æ–∑—ñ {threshold}: {count} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤")
            
            return {
                "collection_count": collection_count,
                "sample_docs": len(sample["documents"]) if sample["documents"] else 0,
                "current_threshold": self.similarity_threshold,
                "test_results_count": len(test_results["documents"][0]) if test_results["documents"] else 0,
                "keywords_extracted": keywords
            }
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ debug: {e}")
            return {"error": str(e)}
    
    def get_stats(self) -> Dict:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –±–∞–∑–∏ –¥–∞–Ω–∏—Ö"""
        try:
            all_data = self.collection.get()
            
            stats = {
                "total_documents": len(all_data["ids"]) if all_data["ids"] else 0,
                "categories": {},
                "sources": {},
                "groups": set(),
                "news_count": 0,
                "schedule_count": 0,
                "similarity_threshold": self.similarity_threshold,
                "avg_content_length": 0
            }
            
            if all_data["metadatas"] and all_data["documents"]:
                total_length = 0
                for i, metadata in enumerate(all_data["metadatas"]):
                    # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó
                    category = metadata.get("category", "unknown")
                    stats["categories"][category] = stats["categories"].get(category, 0) + 1
                    
                    # –î–∂–µ—Ä–µ–ª–∞
                    source = metadata.get("source", "unknown") 
                    stats["sources"][source] = stats["sources"].get(source, 0) + 1
                    
                    # –ì—Ä—É–ø–∏ (–¥–ª—è —Ä–æ–∑–∫–ª–∞–¥—É)
                    if metadata.get("group"):
                        stats["groups"].add(metadata["group"])
                    
                    # –õ—ñ—á–∏–ª—å–Ω–∏–∫–∏ –∑–∞ —Ç–∏–ø–∞–º–∏
                    if source == "news":
                        stats["news_count"] += 1
                    elif source == "portal" or category == "schedule":
                        stats["schedule_count"] += 1
                    
                    # –î–æ–≤–∂–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç—É
                    if i < len(all_data["documents"]):
                        content_length = len(all_data["documents"][i])
                        total_length += content_length
                
                stats["avg_content_length"] = total_length // len(all_data["documents"]) if all_data["documents"] else 0
            
            stats["groups"] = list(stats["groups"])
            stats["unique_categories"] = len(stats["categories"])
            stats["unique_sources"] = len(stats["sources"])
            
            return stats
            
        except Exception as e:
            return {"error": str(e)}
    
    def clear_collection(self, confirm: bool = False):
        """–û—á–∏—â–µ–Ω–Ω—è –∫–æ–ª–µ–∫—Ü—ñ—ó"""
        if not confirm:
            logger.debug("‚ö†Ô∏è –î–ª—è –æ—á–∏—â–µ–Ω–Ω—è –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –ø–µ—Ä–µ–¥–∞–π—Ç–µ confirm=True")
            return
        
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self.client.create_collection(
                name=self.collection_name,
                metadata={"description": "NAU Knowledge Base"}
            )
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–∏—Ö –æ—á–∏—â–µ–Ω–∞")
        except Exception as e:
            logger.critical(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ—á–∏—â–µ–Ω–Ω—è –ë–î: {e}")
    
    def update_document(self, doc_id: str, new_content: str = None, 
                       new_metadata: Dict = None):
        """–û–Ω–æ–≤–ª–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        try:
            result = self.collection.get(ids=[doc_id])
            
            if not result["ids"]:
                print(f"–î–æ–∫—É–º–µ–Ω—Ç {doc_id} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
                return False
            
            content = new_content or result["documents"][0]
            metadata = result["metadatas"][0]
            
            if new_metadata:
                metadata.update(new_metadata)
            
            metadata["updated_at"] = datetime.now().isoformat()
            
            if new_content:
                embedding = self.embedding_model.encode([content]).tolist()[0]
                self.collection.update(
                    ids=[doc_id],
                    embeddings=[embedding],
                    documents=[content],
                    metadatas=[metadata]
                )
            else:
                self.collection.update(
                    ids=[doc_id],
                    metadatas=[metadata]
                )
            
            logger.debug(f"‚úÖ –î–æ–∫—É–º–µ–Ω—Ç {doc_id} –æ–Ω–æ–≤–ª–µ–Ω–æ")
            return True
            
        except Exception as e:
            logger.critical(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –æ–Ω–æ–≤–ª–µ–Ω–Ω—è –¥–æ–∫—É–º–µ–Ω—Ç–∞: {e}")
            return False
    
    def export_to_json(self, output_path: str = "nau_db_export.json"):
        """–ï–∫—Å–ø–æ—Ä—Ç –±–∞–∑–∏ –¥–∞–Ω–∏—Ö –≤ JSON"""
        import json
        
        try:
            all_data = self.collection.get()
            
            export_data = []
            if all_data["ids"]:
                for i in range(len(all_data["ids"])):
                    export_data.append({
                        "id": all_data["ids"][i],
                        "content": all_data["documents"][i] if all_data["documents"] else "",
                        "metadata": all_data["metadatas"][i] if all_data["metadatas"] else {}
                    })
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"‚úÖ –ï–∫—Å–ø–æ—Ä—Ç–æ–≤–∞–Ω–æ {len(export_data)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤ –≤ {output_path}")
            
        except Exception as e:
            logger.critical(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –µ–∫—Å–ø–æ—Ä—Ç—É: {e}")