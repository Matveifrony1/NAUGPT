"""
NAU AI Assistant Backend - Result Validator
–í–∞–ª—ñ–¥–∞—Ü—ñ—è —Ç–∞ —Ä–µ-—Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ—é –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è–º
"""

import httpx
import json
import re
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass
from config import settings
from logger import get_logger

logger = get_logger(__name__)


@dataclass
class ValidationDecision:
    """–†—ñ—à–µ–Ω–Ω—è –≤–∞–ª—ñ–¥–∞—Ç–æ—Ä–∞"""
    is_relevant: bool  # –ß–∏ —î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º–∏
    selected_indices: List[int]  # –Ü–Ω–¥–µ–∫—Å–∏ –æ–±—Ä–∞–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ (1-3)
    confidence: float  # –í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å 0-1
    reasoning: str  # –ü–æ—è—Å–Ω–µ–Ω–Ω—è —Ä—ñ—à–µ–Ω–Ω—è
    needs_reformulation: bool  # –ß–∏ –ø–æ—Ç—Ä—ñ–±–Ω–æ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞—Ç–∏ –∑–∞–ø–∏—Ç
    reformulated_query: Optional[str] = None  # –ù–æ–≤–∏–π –∑–∞–ø–∏—Ç –¥–ª—è embedding —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    reformulation_strategy: Optional[str] = None  # –°—Ç—Ä–∞—Ç–µ–≥—ñ—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è


class ResultValidator:
    """
    –í–∞–ª—ñ–¥–∞—Ç–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –ø–æ—à—É–∫—É –∑ —Ä–æ–∑—É–º–Ω–∏–º –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è–º
    """
    
    def __init__(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è"""
        if settings.USE_GEMINI:
            import google.generativeai as genai
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.gemini_model = genai.GenerativeModel(settings.GEMINI_MODEL)
            self.lm_studio_url = None
        else:
            self.lm_studio_url = "http://localhost:1234/v1/chat/completions"
            self.gemini_model = None
            
        self.max_retries = 2
    
    async def validate_and_select(
        self, 
        original_query: str,
        search_results: List[Dict],
        route_reasoning: str,
        attempt: int = 1
    ) -> Tuple[ValidationDecision, List[Dict]]:
        """
        –í–∞–ª—ñ–¥—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞ –≤–∏–±–∏—Ä–∞—î –Ω–∞–π–∫—Ä–∞—â—ñ
        
        Args:
            original_query: –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∑–∞–ø–∏—Ç –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
            search_results: —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –ë–î (—Ç–æ–ø-6)
            route_reasoning: –ø–æ—è—Å–Ω–µ–Ω–Ω—è –≤—ñ–¥ Query Router –ø—Ä–æ —â–æ –æ—á—ñ–∫—É—î—Ç—å—Å—è
            attempt: –Ω–æ–º–µ—Ä —Å–ø—Ä–æ–±–∏ (–¥–ª—è —Ä–µ–∫—É—Ä—Å—ñ—ó)
        
        Returns:
            (ValidationDecision, –≤—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –ø–æ–≤–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º)
        """
        
        logger.debug(f"üîç –í–ê–õ–Ü–î–ê–¶–Ü–Ø (—Å–ø—Ä–æ–±–∞ {attempt}): query='{original_query[:50]}...', results={len(search_results)}")
        logger.debug(f"üéØ –û–ß–Ü–ö–£–í–ê–ù–ù–Ø ROUTER: {route_reasoning}")
        
        # –ì–æ—Ç—É—î–º–æ —Å–∫–æ—Ä–æ—á–µ–Ω—ñ –≤–µ—Ä—Å—ñ—ó –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        summarized_results = self._summarize_results(search_results)
        
        # –í–∏–∫–ª–∏–∫–∞—î–º–æ LLM –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        decision = await self._llm_validation(
            original_query=original_query,
            summarized_results=summarized_results,
            route_reasoning=route_reasoning,
            attempt=attempt
        )
        
        logger.debug(f"‚úÖ –†–Ü–®–ï–ù–ù–Ø –í–ê–õ–Ü–î–ê–¢–û–†–ê: relevant={decision.is_relevant}, "
              f"selected={decision.selected_indices}, needs_reformulation={decision.needs_reformulation}")
        
        if decision.reformulated_query:
            logger.debug(f"üîÑ –°–¢–†–ê–¢–ï–ì–Ü–Ø: {decision.reformulation_strategy}")
            logger.debug(f"üîÑ –ù–û–í–ò–ô –ó–ê–ü–ò–¢: '{decision.reformulated_query}'")
        
        # –Ø–∫—â–æ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ —Ç–∞ –ø–æ—Ç—Ä—ñ–±–Ω–∞ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –ø—É—Å—Ç—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        # –û—Å–Ω–æ–≤–Ω–∞ –ª–æ–≥—ñ–∫–∞ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ—à—É–∫—É –±—É–¥–µ –≤ assistant.py
        if decision.needs_reformulation and attempt < self.max_retries:
            return decision, []
        
        # –Ø–∫—â–æ –≤—Å–µ –æ–∫ - –ø–æ–≤–µ—Ä—Ç–∞—î–º–æ –æ–±—Ä–∞–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ –ü–û–í–ù–ò–ú –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
        if decision.is_relevant and decision.selected_indices:
            selected_results = [
                search_results[i-1]  # -1 –±–æ LLM –¥–∞—î 1-based —ñ–Ω–¥–µ–∫—Å–∏
                for i in decision.selected_indices 
                if 1 <= i <= len(search_results)
            ]
            return decision, selected_results
        
        # –Ø–∫—â–æ –Ω—ñ—á–æ–≥–æ –Ω–µ –ø—ñ–¥—ñ–π—à–ª–æ –Ω–∞–≤—ñ—Ç—å –ø—ñ—Å–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—è
        logger.error(f"‚ùå –í–ê–õ–Ü–î–ê–¶–Ü–Ø: –∂–æ–¥–Ω–∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return decision, []
    
    def _summarize_results(self, results: List[Dict], max_chars: int = 150) -> List[Dict]:
        """–°—Ç–≤–æ—Ä—é—î —Å–∫–æ—Ä–æ—á–µ–Ω—ñ –≤–µ—Ä—Å—ñ—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É"""
        summarized = []
        
        for i, result in enumerate(results, 1):
            content = result.get("content", "")
            metadata = result.get("metadata", {})
            
            # –°–∫–æ—Ä–æ—á–µ–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
            short_content = content[:max_chars] + "..." if len(content) > max_chars else content
            
            summarized.append({
                "index": i,
                "title": metadata.get("title", "–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞"),
                "content_preview": short_content,
                "faculty": metadata.get("faculty"),
                "department": metadata.get("department"),
                "category": metadata.get("category"),
                "news_type": metadata.get("news_type"),
                "relevance_score": result.get("relevance_score", 0),
                "search_type": result.get("search_type", "unknown")
            })
        
        return summarized
    
    async def _llm_validation(
        self, 
        original_query: str, 
        summarized_results: List[Dict],
        route_reasoning: str,
        attempt: int,
        max_retries: int = 3
    ) -> ValidationDecision:
        """
        –í–∏–∫–ª–∏–∫ LLM –¥–ª—è –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º–∏ retry
        
        Args:
            max_retries: –º–∞–∫—Å–∏–º—É–º –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö JSON (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3)
        """
        
        system_prompt = self._build_validation_prompt()
        
        # –§–æ—Ä–º–∞—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
        results_text = "\n\n".join([
            f"üìÑ –†–ï–ó–£–õ–¨–¢–ê–¢ #{r['index']}:\n"
            f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {r['title']}\n"
            f"–§–∞–∫—É–ª—å—Ç–µ—Ç: {r.get('faculty', '–ù–µ –≤–∫–∞–∑–∞–Ω–æ')}\n"
            f"–ö–∞—Ñ–µ–¥—Ä–∞: {r.get('department', '–ù–µ –≤–∫–∞–∑–∞–Ω–æ')}\n"
            f"–ö–∞—Ç–µ–≥–æ—Ä—ñ—è: {r.get('category', '–ù–µ –≤–∫–∞–∑–∞–Ω–æ')}\n"
            f"–¢–∏–ø: {r.get('news_type', '–ù–µ –≤–∫–∞–∑–∞–Ω–æ')}\n"
            f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ—Å—Ç—å (–ë–î): {r['relevance_score']:.2f}\n"
            f"–¢–∏–ø –ø–æ—à—É–∫—É: {r.get('search_type', 'unknown')}\n"
            f"–ü—Ä–µ–≤—å—é –∫–æ–Ω—Ç–µ–Ω—Ç—É:\n{r['content_preview']}"
            for r in summarized_results
        ])
        
        user_message = f"""–û–†–ò–ì–Ü–ù–ê–õ–¨–ù–ò–ô –ó–ê–ü–ò–¢ –ö–û–†–ò–°–¢–£–í–ê–ß–ê: "{original_query}"

    –©–û –û–ß–Ü–ö–£–Ñ–¢–¨–°–Ø (–≤—ñ–¥ Query Router):
    {route_reasoning}

    –°–ü–†–û–ë–ê –ü–û–®–£–ö–£: {attempt} –∑ {self.max_retries}

    –†–ï–ó–£–õ–¨–¢–ê–¢–ò –ü–û–®–£–ö–£ –í –ë–î ({len(summarized_results)} –¥–æ–∫—É–º–µ–Ω—Ç—ñ–≤):

    {results_text}

    ---

    –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π —á–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—é—Ç—å —Ü—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –û–ß–Ü–ö–£–í–ê–ù–ù–Ø–ú –≤—ñ–¥ Query Router —Ç–∞ –ó–ê–ü–ò–¢–£ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞.

    –í–ê–ñ–õ–ò–í–û:
    - –Ø–∫—â–æ –∂–æ–¥–Ω–æ–≥–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É - –ü–ï–†–ï–§–û–†–ú–£–õ–Æ–ô –∑–∞–ø–∏—Ç –¥–ª—è embedding
    - –ü—Ä–∏ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª—é–≤–∞–Ω–Ω—ñ –°–ü–†–û–©–£–ô: –≤–∏–¥–∞–ª—è–π –∑–∞–π–≤—ñ —Å–ª–æ–≤–∞ ("–∫–æ–Ω—Ç–∞–∫—Ç–∏", "—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"), –∑–∞–ª–∏—à–∞–π —Å—É—Ç—å (—ñ–º'—è, –Ω–∞–∑–≤—É)
    - –Ø–∫—â–æ —à—É–∫–∞–ª–∏ "–∫–æ–Ω—Ç–∞–∫—Ç–∏ –Ü–≤–∞–Ω–æ–≤–∞" —ñ –Ω–µ –∑–Ω–∞–π—à–ª–∏ - —à—É–∫–∞–π –ø—Ä–æ—Å—Ç–æ "–Ü–≤–∞–Ω–æ–≤ –≤–∏–∫–ª–∞–¥–∞—á"
    - –Ø–∫—â–æ —à—É–∫–∞–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É –ø–æ–¥—ñ—é —ñ –Ω–µ –∑–Ω–∞–π—à–ª–∏ - —à—É–∫–∞–π –±—ñ–ª—å—à –∑–∞–≥–∞–ª—å–Ω–æ

    –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π –¢–Ü–õ–¨–ö–ò JSON:
    {{
      "reasoning": "–ü–æ—è—Å–Ω–µ–Ω–Ω—è",
      "is_relevant": true/false,
      "selected_indices": [1, 2],
      "confidence": 0.85,
      "needs_reformulation": false,
      "reformulated_query": null,
      "reformulation_strategy": null
    }}"""

        # ‚úÖ RETRY LOOP
        for retry_attempt in range(1, max_retries + 1):
            try:
                logger.debug(f"ü§ñ –í–ê–õ–Ü–î–ê–¢–û–† LLM: —Å–ø—Ä–æ–±–∞ {retry_attempt}/{max_retries}")
                
                # ========== –ó–ê–ú–Ü–ù–ê –ù–ê GEMINI ==========
                if settings.USE_GEMINI:
                    import google.generativeai as genai
                    
                    full_prompt = f"{system_prompt}\n\n{user_message}"
                    
                    response = self.gemini_model.generate_content(
                        full_prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=0.3,
                            max_output_tokens=1000,
                        )
                    )
                    
                    llm_response = response.text
                    logger.debug(f"ü§ñ –í–ê–õ–Ü–î–ê–¢–û–† –í–Ü–î–ü–û–í–Ü–í: {llm_response}")
                    
                    # ‚úÖ –ü–ê–†–°–ò–ù–ì JSON
                    parsed = self._extract_json_from_llm(llm_response)
                    
                    if parsed:
                        # ‚úÖ –£–°–ü–ï–®–ù–û –†–ê–°–ü–ê–†–°–ò–õ–ò
                        logger.debug(f"‚úÖ JSON —É—Å–ø—ñ—à–Ω–æ —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏–π –Ω–∞ —Å–ø—Ä–æ–±—ñ {retry_attempt}")
                        
                        return ValidationDecision(
                            is_relevant=parsed.get("is_relevant", False),
                            selected_indices=parsed.get("selected_indices", []),
                            confidence=parsed.get("confidence", 0.0),
                            reasoning=parsed.get("reasoning", ""),
                            needs_reformulation=parsed.get("needs_reformulation", False),
                            reformulated_query=parsed.get("reformulated_query"),
                            reformulation_strategy=parsed.get("reformulation_strategy")
                        )
                    else:
                        # ‚ùå –ù–ï –°–ú–û–ì–õ–ò –†–ê–°–ü–ê–†–°–ò–¢–¨ - RETRY
                        logger.warning(f"‚ö†Ô∏è –í–∞–ª—ñ–¥–∞—Ç–æ—Ä (—Å–ø—Ä–æ–±–∞ {retry_attempt}): –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON")
                        if retry_attempt < max_retries:
                            logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó...")
                            continue  # ‚Üê –ü–û–í–¢–û–†–Ø–ï–ú
                        else:
                            # ‚ùå –í–°–ï –ü–û–ü–´–¢–ö–ò –ü–†–û–í–ê–õ–ò–õ–ò–°–¨ - FALLBACK
                            logger.error(f"‚ùå –í–∏—á–µ—Ä–ø–∞–Ω–æ —Å–ø—Ä–æ–±–∏ ({max_retries}), fallback –Ω–∞ –±–∞–∑–æ–≤–µ —Ä—ñ—à–µ–Ω–Ω—è")
                            return ValidationDecision(
                                is_relevant=len(summarized_results) > 0,
                                selected_indices=[1, 2, 3] if len(summarized_results) >= 3 else list(range(1, len(summarized_results) + 1)),
                                confidence=0.3,
                                reasoning="Fallback: LLM –Ω–µ –∑–º—ñ–≥ —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON –ø—ñ—Å–ª—è –≤—Å—ñ—Ö —Å–ø—Ä–æ–±",
                                needs_reformulation=False,
                                reformulated_query=None,
                                reformulation_strategy=None
                            )
                
                # ========== –°–¢–ê–†–ò–ô –ö–û–î –î–õ–Ø LM STUDIO ==========
                else:
                    async with httpx.AsyncClient(timeout=300.0) as client:
                        response = await client.post(
                            self.lm_studio_url,
                            json={
                                "messages": [
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": user_message}
                                ],
                                "temperature": 0.3,
                                "max_tokens": 1000
                            }
                        )
                        
                        if response.status_code == 200:
                            data = response.json()
                            llm_response = data["choices"][0]["message"]["content"]
                            logger.debug(f"ü§ñ –í–ê–õ–Ü–î–ê–¢–û–† –í–Ü–î–ü–û–í–Ü–í: {llm_response[:300]}...")
                            
                            # ‚úÖ –ü–ê–†–°–ò–ù–ì JSON
                            parsed = self._extract_json_from_llm(llm_response)
                            
                            if parsed:
                                # ‚úÖ –£–°–ü–ï–®–ù–û –†–ê–°–ü–ê–†–°–ò–õ–ò
                                logger.debug(f"‚úÖ JSON —É—Å–ø—ñ—à–Ω–æ —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏–π –Ω–∞ —Å–ø—Ä–æ–±—ñ {retry_attempt}")
                                
                                return ValidationDecision(
                                    is_relevant=parsed.get("is_relevant", False),
                                    selected_indices=parsed.get("selected_indices", []),
                                    confidence=parsed.get("confidence", 0.0),
                                    reasoning=parsed.get("reasoning", ""),
                                    needs_reformulation=parsed.get("needs_reformulation", False),
                                    reformulated_query=parsed.get("reformulated_query"),
                                    reformulation_strategy=parsed.get("reformulation_strategy")
                                )
                            else:
                                # ‚ùå –ù–ï –°–ú–û–ì–õ–ò –†–ê–°–ü–ê–†–°–ò–¢–¨ - RETRY
                                logger.warning(f"‚ö†Ô∏è –í–∞–ª—ñ–¥–∞—Ç–æ—Ä (—Å–ø—Ä–æ–±–∞ {retry_attempt}): –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON")
                                if retry_attempt < max_retries:
                                    logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó...")
                                    continue  # ‚Üê –ü–û–í–¢–û–†–Ø–ï–ú
                                else:
                                    # ‚ùå –í–°–ï –ü–û–ü–´–¢–ö–ò –ü–†–û–í–ê–õ–ò–õ–ò–°–¨ - FALLBACK
                                    logger.error(f"‚ùå –í–∏—á–µ—Ä–ø–∞–Ω–æ —Å–ø—Ä–æ–±–∏ ({max_retries}), fallback –Ω–∞ –±–∞–∑–æ–≤–µ —Ä—ñ—à–µ–Ω–Ω—è")
                                    return ValidationDecision(
                                        is_relevant=len(summarized_results) > 0,
                                        selected_indices=[1, 2, 3] if len(summarized_results) >= 3 else list(range(1, len(summarized_results) + 1)),
                                        confidence=0.3,
                                        reasoning="Fallback: LLM –Ω–µ –∑–º—ñ–≥ —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON –ø—ñ—Å–ª—è –≤—Å—ñ—Ö —Å–ø—Ä–æ–±",
                                        needs_reformulation=False,
                                        reformulated_query=None,
                                        reformulation_strategy=None
                                    )
                        else:
                            logger.warning(f"‚ö†Ô∏è –í–∞–ª—ñ–¥–∞—Ç–æ—Ä (—Å–ø—Ä–æ–±–∞ {retry_attempt}): HTTP {response.status_code}")
                            if retry_attempt < max_retries:
                                continue
                            # Fallback
                            return ValidationDecision(
                                is_relevant=False,
                                selected_indices=[],
                                confidence=0.0,
                                reasoning=f"HTTP error {response.status_code}",
                                needs_reformulation=False
                            )
                            
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –í–∞–ª—ñ–¥–∞—Ç–æ—Ä (—Å–ø—Ä–æ–±–∞ {retry_attempt}): –ø–æ–º–∏–ª–∫–∞ {e}")
                if retry_attempt < max_retries:
                    logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞ –≤–∞–ª—ñ–¥–∞—Ü—ñ—ó...")
                    continue
                # Fallback –Ω–∞ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Å–ø—Ä–æ–±—ñ
                return ValidationDecision(
                    is_relevant=False,
                    selected_indices=[],
                    confidence=0.0,
                    reasoning=f"Exception: {str(e)}",
                    needs_reformulation=False
                )
        
        # –¶–µ–π –∫–æ–¥ –Ω—ñ–∫–æ–ª–∏ –Ω–µ –≤–∏–∫–æ–Ω–∞—î—Ç—å—Å—è, –∞–ª–µ –¥–ª—è –±–µ–∑–ø–µ–∫–∏
        return ValidationDecision(
            is_relevant=False,
            selected_indices=[],
            confidence=0.0,
            reasoning="Unknown error in retry loop",
            needs_reformulation=False
        )
    
    def _build_validation_prompt(self) -> str:
        return """You are an expert validation system for NAU search results.

YOUR TASK:
1. Analyze if DB results ACTUALLY match what is expected
2. Select 1-3 BEST results for response
3. If results are irrelevant - REFORMULATE query for embedding search

‚îÅ‚îÅ‚îÅ‚îÅ
üéØ VALIDATION RULES
‚îÅ‚îÅ‚îÅ‚îÅ

‚úÖ RESULT IS RELEVANT IF:
- Matches query TOPIC and Router EXPECTATIONS
- Contains information about CORRECT person/event/place
- Correct CONTEXT (faculty/department)
- Not outdated (if query is about "recent")

‚ùå RESULT IS IRRELEVANT IF:
- Completely different topic
- Different person with same name
- Wrong faculty/department
- Outdated data when fresh is needed

‚îÅ‚îÅ‚îÅ‚îÅ
üîÑ REFORMULATION STRATEGIES (CRITICALLY IMPORTANT!)
‚îÅ‚îÅ‚îÅ‚îÅ

Embedding search works POORLY with overly specific queries!

üö´ DOESN'T WORK: "–∫–æ–Ω—Ç–∞–∫—Ç–∏ –Ü–≤–∞–Ω–æ–≤–∞ –ü–µ—Ç—Ä–∞ –°—Ç–µ–ø–∞–Ω–æ–≤–∏—á–∞ email —Ç–µ–ª–µ—Ñ–æ–Ω –∫–∞—Ñ–µ–¥—Ä–∞ –Ü–ü–ó"
‚úÖ WORKS: "–Ü–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä–æ –°—Ç–µ–ø–∞–Ω–æ–≤–∏—á –∫–∞—Ñ–µ–¥—Ä–∞ –≤–∏–∫–ª–∞–¥–∞—á"

üö´ DOESN'T WORK: "—Ä–µ–∫–≤—ñ–∑–∏—Ç–∏ –¥–ª—è –æ–ø–ª–∞—Ç–∏ –Ω–∞–≤—á–∞–Ω–Ω—è –±–∞–Ω–∫—ñ–≤—Å—å–∫—ñ –¥–∞–Ω—ñ —Ä–∞—Ö—É–Ω–æ–∫"
‚úÖ WORKS: "–æ–ø–ª–∞—Ç–∞ –Ω–∞–≤—á–∞–Ω–Ω—è —Ä–µ–∫–≤—ñ–∑–∏—Ç–∏ –±–∞–Ω–∫—ñ–≤—Å—å–∫—ñ"

üö´ DOESN'T WORK: "–æ—Å—Ç–∞–Ω–Ω—è –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –ø–æ–ª—ñ—Ç –¥–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è —É—á–∞—Å–Ω–∏–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–∞"
‚úÖ WORKS: "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –ø–æ–ª—ñ—Ç"

**GOLDEN RULE:** SIMPLER and MORE GENERAL ‚Üí BETTER embedding will find!

STRATEGIES:

1. **"enrich_person"** - enrich for people
   - Add: "–≤–∏–∫–ª–∞–¥–∞—á –¥–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ñ–µ—Å–æ—Ä –ø–µ—Ä—Å–æ–Ω–∞–ª –∑–∞–≤—ñ–¥—É–≤–∞—á"
   - Add faculty/department if available
   - "–ú–∞–ª—è—Ä—á—É–∫" ‚Üí "–ú–∞–ª—è—Ä—á—É–∫ –≤–∏–∫–ª–∞–¥–∞—á –¥–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ñ–µ—Å–æ—Ä"

2. **"enrich_event"** - enrich for events
   - Add: "–ø–æ–¥—ñ—è –∑–∞—Ö—ñ–¥ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è —Å–µ–º—ñ–Ω–∞—Ä"
   - "–ø–æ–ª—ñ—Ç" ‚Üí "–ø–æ–ª—ñ—Ç –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –∑–∞—Ö—ñ–¥ –ù–ê–£ –§–ö–ù–¢"

3. **"enrich_contacts"** - enrich contacts
   - Add: "–∫–æ–Ω—Ç–∞–∫—Ç–∏ email —Ç–µ–ª–µ—Ñ–æ–Ω –∑–≤'—è–∑–æ–∫"
   - "–ö–°–ú" ‚Üí "–ö–°–ú –∫–æ–Ω—Ç–∞–∫—Ç–∏ email —Ç–µ–ª–µ—Ñ–æ–Ω –∫–∞—Ñ–µ–¥—Ä–∞"

4. **"generalize_topic"** - generalize topic
   - Was: "—Å—Ç–∏–ø–µ–Ω–¥—ñ—è —Å–æ—Ü—ñ–∞–ª—å–Ω–∞ –¥–æ–ø–æ–º–æ–≥–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞ —Å—Ç—É–¥–µ–Ω—Ç—ñ–≤ —É–º–æ–≤–∏"
   - Became: "—Å—Ç–∏–ø–µ–Ω–¥—ñ—è —Å—Ç—É–¥–µ–Ω—Ç–∏"

5. **"expand_scope"** - expand scope
   - When: confidence < 0.7 or empty
   - Remove narrow constraints, add broader context
   - "–¢—É—Ä—É–π –ö–°–ú —Å—Ç–∞—Ç—Ç—ñ" ‚Üí "–¢—É—Ä—É–π –§–ö–ù–¢ –≤–∏–∫–ª–∞–¥–∞—á –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ"

6. **"add_synonyms"** - add topic synonyms
   - "—Å—Ç–∞—Ç—Ç—ñ" = "–ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –Ω–∞—É–∫–æ–≤—ñ —Ä–æ–±–æ—Ç–∏"
   - "–∫–æ–Ω—Ç–∞–∫—Ç–∏" = "email —Ç–µ–ª–µ—Ñ–æ–Ω –∑–≤'—è–∑–æ–∫ –∞–¥—Ä–µ—Å–∞"

‚îÅ‚îÅ‚îÅ‚îÅ
üìã EXAMPLES
‚îÅ‚îÅ‚îÅ‚îÅ

EXAMPLE 1: Teacher not found
Query: "—Å–∫–∞–∂–∏ –µ–≥–æ –Ω–æ–º–µ—Ä"
Router Expectations: "–ö–æ–Ω—Ç–∞–∫—Ç–∏ –ú–∞–ª—è—Ä—á—É–∫–∞ –∑ –ö–°–ú"
Router confidence: 0.75
Results: random news, not about Malyarchuk

Decision:
{
  "reasoning": "Analyzing search results... See that Router expects Malyarchuk's contacts from KSM. Looking at results: there's news about KSM, but nothing about Malyarchuk. This means query was too complex for embedding. Solution: simplifying query - adding synonyms '–≤–∏–∫–ª–∞–¥–∞—á', '–¥–æ—Ü–µ–Ω—Ç', '–ø—Ä–æ—Ñ–µ—Å–æ—Ä' and context '–ö–°–ú –§–ö–ù–¢'. Also adding word '–∫–æ–Ω—Ç–∞–∫—Ç–∏' to find contact info.",
  "is_relevant": false,
  "selected_indices": [],
  "confidence": 0.2,
  "needs_reformulation": true,
  "reformulated_query": "–ú–∞–ª—è—Ä—á—É–∫ –≤–∏–∫–ª–∞–¥–∞—á –¥–æ—Ü–µ–Ω—Ç –ø—Ä–æ—Ñ–µ—Å–æ—Ä –ö–°–ú –§–ö–ù–¢ –ø–µ—Ä—Å–æ–Ω–∞–ª –∫–æ–Ω—Ç–∞–∫—Ç–∏",
  "reformulation_strategy": "enrich_person"
}

---

EXAMPLE 2: Article not found - narrow scope
Query: "–Ω–∞–π–¥–∏ —Å—Ç–∞—Ç—Ç—é –¢—É—Ä—É—è"
Router Expectations: "–ü—É–±–ª—ñ–∫–∞—Ü—ñ—ó –¢—É—Ä—É—è –∑ –ö–°–ú"
Router confidence: 0.6, target_entity: "–ö–°–ú"
Results: general info about KSM, nothing about articles

Decision:
{
  "reasoning": "Thinking... Router has low confidence (0.6) and limited search to KSM only. Results: there are materials about department, but no mention of Turuy's articles. Maybe Turuy is not from KSM, or his articles are not within department scope. Solution: expanding scope from KSM to entire FKNT + adding synonyms '–ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó', '–¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è', '–Ω–∞—É–∫–æ–≤—ñ —Ä–æ–±–æ—Ç–∏'. This will increase chances of finding.",
  "is_relevant": false,
  "selected_indices": [],
  "confidence": 0.3,
  "needs_reformulation": true,
  "reformulated_query": "–¢—É—Ä—É–π –§–ö–ù–¢ –≤–∏–∫–ª–∞–¥–∞—á –ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó –¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è —Å—Ç–∞—Ç—Ç—ñ –Ω–∞—É–∫–æ–≤—ñ —Ä–æ–±–æ—Ç–∏",
  "reformulation_strategy": "expand_scope"
}

---

EXAMPLE 3: Found event, but few details
Query: "–∫–æ–ª–∏ –±—É–ª–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –ø–æ–ª—ñ—Ç"
Results:
  #1 - Brief mention of Polit in FKNT news
  #2 - Department history (has word "–ø–æ–ª—ñ—Ç")
  #3 - Sports news

Decision:
{
  "reasoning": "Analyzing results... Result #1 mentions Polit, but very briefly. Result #2 - not about conference at all. Result #3 - off topic. Conclusion: embedding found something, but too superficially. Need details about event - date, program. Solution: enriching query with event context + synonyms '–∑–∞—Ö—ñ–¥', '—Å–µ–º—ñ–Ω–∞—Ä', adding that it's –ù–ê–£ –§–ö–ù–¢.",
  "is_relevant": false,
  "selected_indices": [],
  "confidence": 0.4,
  "needs_reformulation": true,
  "reformulated_query": "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è –ø–æ–ª—ñ—Ç –∑–∞—Ö—ñ–¥ —Å–µ–º—ñ–Ω–∞—Ä –ù–ê–£ –§–ö–ù–¢ –¥–∞—Ç–∞ –ø—Ä–æ–≥—Ä–∞–º–∞",
  "reformulation_strategy": "enrich_event"
}

---

EXAMPLE 4: Found CORRECTLY - no need to reformulate
Query: "–∫–æ–Ω—Ç–∞–∫—Ç–∏ –Ü–≤–∞–Ω–æ–≤–∞"
Results:
  #1 - Ivanov P.S., head of IPZ, email: ivanov@..., tel: +380...
  #2 - Ivanova O.M., associate professor KIT
  #3 - IPZ history

Decision:
{
  "reasoning": "Looking at results... Result #1 - PERFECT: Ivanov P.S., head of IPZ, has full contacts (email, phone). This is exactly what's needed! Result #2 - different person (Ivanova is female). Result #3 - historical reference, not relevant. Conclusion: first result fully matches query, additional ones not needed. Selecting only #1.",
  "is_relevant": true,
  "selected_indices": [1],
  "confidence": 0.95,
  "needs_reformulation": false,
  "reformulated_query": null,
  "reformulation_strategy": null
}

‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è CRITICALLY IMPORTANT: RESPONSE FORMAT
‚îÅ‚îÅ‚îÅ‚îÅ

üéØ **MANDATORY FIELD ORDER:**

1. **reasoning** - ALWAYS FIRST FIELD! Here you think aloud BEFORE making decision
2. Then all other fields

**JSON FORMAT (reasoning FIRST!):**

{
  "reasoning": "Here I think aloud: what I see in results, why relevant/irrelevant, what decision I make and why...",
  "is_relevant": true,
  "selected_indices": [1, 2],
  "confidence": 0.85,
  "needs_reformulation": false,
  "reformulated_query": null,
  "reformulation_strategy": null
}

**HOW TO WRITE REASONING:**

‚úÖ GOOD: "Analyzing results... See that #1 contains Malyarchuk's email, #2 about KSM structure. Query was about contacts ‚Üí choosing #1 because it has needed info. Confidence high due to exact match."

‚ùå BAD: "Result is relevant" (too short, no reasoning)

**REASONING MUST ANSWER:**
- What do I see in results?
- Does this match Router expectations?
- Why am I choosing these indices (or why not)?
- If reformulating - why exactly this way?

‚îÅ‚îÅ‚îÅ‚îÅ
KEYWORD EXTRACTION RULES:
‚îÅ‚îÅ‚îÅ‚îÅ
Extract ONLY content words that would appear in news articles:
‚úÖ Include: nouns, specific terms, names, topics ("—Ñ—É—Ç–±–æ–ª", "–º–∞—Ç—á", "–ù–ê–£", "—Å–µ–º—ñ–Ω–∞—Ä", "AI")
‚ùå Exclude: question words ("–∫–æ–ª–∏", "–¥–µ", "—â–æ"), verbs ("–±—É–≤", "–±—É–¥–µ"), temporal markers ("–≤—á–æ—Ä–∞", "–∑–∞–≤—Ç—Ä–∞")

Example:
Query: "–∫–æ–ª–∏ –±—É–≤ –º–∞—Ç—á –≤ —Ñ—É—Ç–±–æ–ª?"
‚ùå Bad keywords: ["–∫–æ–ª–∏", "–±—É–≤", "–º–∞—Ç—á", "—Ñ—É—Ç–±–æ–ª"]
‚úÖ Good keywords: ["—Ñ—É—Ç–±–æ–ª", "–º–∞—Ç—á", "–ù–ê–£", "—Å–ø–æ—Ä—Ç", "–∑–º–∞–≥–∞–Ω–Ω—è"]

Reason: Database searches CONTENT, not questions. Extract what user wants to FIND, not how they ASK.

‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ
GENERATE KEYWORD QUERIES THAT COULD BE USED ON A WEBSITE RELATED TO THE TOPIC OF THE NEWS AND CONNECTED TO ITS TITLE.

Respond with ONLY clean JSON, no markdown, no explanations BEFORE or AFTER JSON!"""
    
    def _extract_json_from_llm(self, text: str) -> Optional[Dict]:
        """–í–∏—Ç—è–≥—É—î JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ LLM"""
        
        # –ü—Ä–æ–±—É—î–º–æ –∑–Ω–∞–π—Ç–∏ JSON –±–ª–æ–∫
        json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
        if json_match:
            try:
                json_str = json_match.group(0)
                # –û—á–∏—â–∞—î–º–æ –≤—ñ–¥ markdown —è–∫—â–æ —î
                json_str = re.sub(r'^```json\s*', '', json_str)
                json_str = re.sub(r'\s*```$', '', json_str)
                return json.loads(json_str)
            except json.JSONDecodeError as e:
                logger.error(f"‚ö†Ô∏è JSON parse error: {e}")
                logger.error(f"‚ö†Ô∏è JSON string: {json_str[:200]}")
        
        return None


# –ï–∫—Å–ø–æ—Ä—Ç
__all__ = ['ResultValidator', 'ValidationDecision']