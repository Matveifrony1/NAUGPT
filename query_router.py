"""
NAU AI Assistant Backend - Query Router
–£–º–Ω–∞—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
"""

import httpx
import json
import re
from typing import Dict, List, Optional
from pydantic import BaseModel, Field
from nau_structure import NAU_STRUCTURE, find_entity_by_alias, extract_entities_from_text
from config import settings
from logger import get_logger

logger = get_logger(__name__)


class RouteDecision(BaseModel):
    """–†—ñ—à–µ–Ω–Ω—è –ø—Ä–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—é –∑–∞–ø–∏—Ç—É"""
    search_scope: str = Field(..., description="–§–∞–∫—É–ª—å—Ç–µ—Ç (–§–ö–ù–¢/–§–ê–ï–¢) –∞–±–æ global")
    search_level: str = Field(..., description="faculty/department/person/general")
    target_entity: Optional[str] = Field(None, description="–ö–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –∫–∞—Ñ–µ–¥—Ä–∞ (–Ü–ü–ó/–ö–Ü–¢/...)")
    search_intent: str = Field(..., description="info/schedule/news/contacts/events")
    enhancement_keywords: List[str] = Field(default=[], description="–î–æ–¥–∞—Ç–∫–æ–≤—ñ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞")
    confidence: float = Field(..., description="–í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å —É —Ä—ñ—à–µ–Ω–Ω—ñ 0-1")
    reasoning: str = Field(..., description="–ü–æ—è—Å–Ω–µ–Ω–Ω—è —Ä—ñ—à–µ–Ω–Ω—è")
    needs_database_search: bool = Field(..., description="–ß–∏ –ø–æ—Ç—Ä—ñ–±–µ–Ω –ø–æ—à—É–∫ —É –±–∞–∑—ñ –¥–∞–Ω–∏—Ö")

class QueryRouter:
    """
    –†–æ–∑—É–º–Ω–∏–π —Ä–æ—É—Ç–µ—Ä –∑–∞–ø–∏—Ç—ñ–≤ –∑ LLM-–∞–Ω–∞–ª—ñ–∑–æ–º
    """
    
    def __init__(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è"""
        if settings.USE_GEMINI:
            import google.generativeai as genai
            genai.configure(api_key=settings.GEMINI_API_KEY)
            self.gemini_model = genai.GenerativeModel(settings.GEMINI_MODEL)
            self.lm_studio_url = None
        else:
            self.lm_studio_url = "http://localhost:1234/v1/chat/completions"
            self.gemini_model = None
            
        self.nau_structure = NAU_STRUCTURE
            
        # –°–ª–æ–≤–Ω–∏–∫ —Å–º–∏—Å–ª–æ–≤–∏—Ö —Ä–æ–∑—à–∏—Ä–µ–Ω—å
        self.semantic_expansions = {
            # –ü–æ–¥—ñ—ó —Ç–∞ –∑—É—Å—Ç—Ä—ñ—á—ñ
            "–ø–æ–ª—ñ—Ç": ["–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è", "–∑–∞—Ö—ñ–¥", "–∑—É—Å—Ç—Ä—ñ—á", "—Å–µ–º—ñ–Ω–∞—Ä", "—Ñ–æ—Ä—É–º", "–∑–±–æ—Ä–∏"],
            "–∑—É—Å—Ç—Ä—ñ—á": ["–∑–∞—Å—ñ–¥–∞–Ω–Ω—è", "–Ω–∞—Ä–∞–¥–∞", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è", "—Å–µ–º—ñ–Ω–∞—Ä"],
            "–ø–æ–¥—ñ—è": ["–∑–∞—Ö—ñ–¥", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è", "—Ñ–æ—Ä—É–º", "—Å–µ–º—ñ–Ω–∞—Ä", "—Å–≤—è—Ç–∫—É–≤–∞–Ω–Ω—è"],
            
            # –í–∏–∫–ª–∞–¥–∞—á—ñ —Ç–∞ –ª—é–¥–∏
            "–≤–∏–∫–ª–∞–¥–∞—á": ["–ø—Ä–æ—Ñ–µ—Å–æ—Ä", "–¥–æ—Ü–µ–Ω—Ç", "–∑–∞–≤—ñ–¥—É–≤–∞—á", "–Ω–∞—É–∫–æ–≤–µ—Ü—å", "–ø–µ–¥–∞–≥–æ–≥"],
            "–∑–∞–≤—ñ–¥—É–≤–∞—á": ["–∑–∞–≤–∫–∞—Ñ–µ–¥—Ä–∏", "–∫–µ—Ä—ñ–≤–Ω–∏–∫ –∫–∞—Ñ–µ–¥—Ä–∏", "–¥–µ–∫–∞–Ω", "–ø—Ä–æ—Ñ–µ—Å–æ—Ä"],
            
            # –ù–∞–≤—á–∞–Ω–Ω—è
            "–Ω–∞–≤—á–∞–Ω–Ω—è": ["–æ—Å–≤—ñ—Ç–∞", "–∑–∞–Ω—è—Ç—Ç—è", "–ø–∞—Ä–∏", "–ª–µ–∫—Ü—ñ—ó", "—Å–µ–º—ñ–Ω–∞—Ä–∏", "–∫—É—Ä—Å"],
            "—Ä–æ–∑–∫–ª–∞–¥": ["–ø–∞—Ä–∏", "–∑–∞–Ω—è—Ç—Ç—è", "–≥—Ä–∞—Ñ—ñ–∫", "—á–∞—Å –∑–∞–Ω—è—Ç—å"],
            
            # –ù–∞—É–∫–∞
            "–¥–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è": ["–Ω–∞—É–∫–∞", "–Ω–∞—É–∫–æ–≤–∞ —Ä–æ–±–æ—Ç–∞", "–ø—É–±–ª—ñ–∫–∞—Ü—ñ—ó", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—ó"],
            "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è": ["—Å–∏–º–ø–æ–∑—ñ—É–º", "—Ñ–æ—Ä—É–º", "–Ω–∞—É–∫–æ–≤–∞ –ø–æ–¥—ñ—è", "—Å–µ–º—ñ–Ω–∞—Ä"],
            
            # –í—Å—Ç—É–ø
            "–≤—Å—Ç—É–ø": ["–∞–±—ñ—Ç—É—Ä—ñ—î–Ω—Ç", "–ø—Ä–∏–π–æ–º", "–¥–æ–∫—É–º–µ–Ω—Ç–∏", "–∫–æ–Ω–∫—É—Ä—Å", "–∑–∞—Ä–∞—Ö—É–≤–∞–Ω–Ω—è"],
            
            # –ö–æ–Ω—Ç–∞–∫—Ç–∏
            "–∫–æ–Ω—Ç–∞–∫—Ç–∏": ["—Ç–µ–ª–µ—Ñ–æ–Ω", "–∞–¥—Ä–µ—Å–∞", "email", "–∑–≤'—è–∑–æ–∫", "—Ä–æ–∑—Ç–∞—à—É–≤–∞–Ω–Ω—è"],
        }
    
    async def route_query(self, query: str, history: Optional[List[Dict]] = None,
                         group_name: Optional[str] = None) -> RouteDecision:
        
        logger.debug(f"üß≠ QUERY ROUTER: –ê–Ω–∞–ª—ñ–∑ –∑–∞–ø–∏—Ç—É '{query[:50]}...'")
        
        # –ó–∞–≤–∂–¥–∏ –≤–∏–∫–ª–∏–∫–∞—î–º–æ LLM
        llm_decision = await self._llm_routing(query, history, group_name)
        
        if llm_decision:
            logger.debug(f"üß≠ LLM –†–Ü–®–ï–ù–ù–Ø: scope={llm_decision.search_scope}, entity={llm_decision.target_entity}")
            return llm_decision
        
        # Fallback –Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫—É —Ç—ñ–ª—å–∫–∏ —è–∫—â–æ LLM –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–≤
        logger.error(f"‚ö†Ô∏è LLM –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π ‚Üí –µ–≤—Ä–∏—Å—Ç–∏—á–Ω–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—è")
        heuristic_decision = self._heuristic_routing(query, history, group_name)
        logger.error(f"üß≠ HEURISTIC: scope={heuristic_decision.search_scope}")
        return heuristic_decision
    
    def _heuristic_routing(self, query: str, history: Optional[List[Dict]], 
                          group_name: Optional[str]) -> RouteDecision:
        """
        –®–≤–∏–¥–∫–∞ –µ–≤—Ä–∏—Å—Ç–∏—á–Ω–∞ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—è (–±–µ–∑ LLM)
        """
        
        query_lower = query.lower()
        
        # === –í–ò–õ–£–ß–ï–ù–ù–Ø –°–£–¢–ù–û–°–¢–ï–ô –ó –ó–ê–ü–ò–¢–£ ===
        entities = extract_entities_from_text(query)
        
        # === –ê–ù–ê–õ–Ü–ó –Ü–°–¢–û–†–Ü–á ===
        context_scope = None
        context_entity = None
        
        if history:
            # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ 3 –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è
            recent_history = history[-6:] if len(history) > 6 else history
            history_text = " ".join([msg["content"] for msg in recent_history])
            history_entities = extract_entities_from_text(history_text)
            
            if history_entities:
                last_entity = history_entities[-1]
                if last_entity["type"] == "faculty":
                    context_scope = last_entity["code"]
                elif last_entity["type"] == "department":
                    context_scope = last_entity["faculty_code"]
                    context_entity = last_entity["code"]
        
        # === –ü–†–Ü–û–†–ò–¢–ï–¢–ò: –ü—Ä—è–º–∞ –∑–≥–∞–¥–∫–∞ > –Ü—Å—Ç–æ—Ä—ñ—è > –ì—Ä—É–ø–∞ ===
        
        # 1. –ü–†–Ø–ú–ï –ó–ì–ê–î–ê–ù–ù–Ø –≤ –∑–∞–ø–∏—Ç—ñ
        search_scope = "global"
        search_level = "general"
        target_entity = None
        confidence = 0.5
        
        if entities:
            entity = entities[0]  # –ë–µ—Ä—ñ–º–æ –ø–µ—Ä—à—É –∑–Ω–∞–π–¥–µ–Ω—É
            if entity["type"] == "faculty":
                search_scope = entity["code"]
                search_level = "faculty"
                confidence = 0.95
            elif entity["type"] == "department":
                search_scope = entity["faculty_code"]
                search_level = "department"
                target_entity = entity["code"]
                confidence = 0.98
        
        # 2. –ö–û–ù–¢–ï–ö–°–¢ –ó –Ü–°–¢–û–†–Ü–á (—è–∫—â–æ –Ω–µ –∑–Ω–∞–π—à–ª–∏ –≤ –∑–∞–ø–∏—Ç—ñ)
        elif context_scope:
            search_scope = context_scope
            if context_entity:
                search_level = "department"
                target_entity = context_entity
                confidence = 0.75
            else:
                search_level = "faculty"
                confidence = 0.70
        
        # 3. –ì–†–£–ü–ê –°–¢–£–î–ï–ù–¢–ê
        elif group_name:
            # –°–ø—Ä–æ–±—É—î–º–æ –≤–∏–∑–Ω–∞—á–∏—Ç–∏ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç –∑–∞ –≥—Ä—É–ø–æ—é (–∑–∞ –ø–∞—Ç–µ—Ä–Ω–∞–º–∏ –∫–æ–¥—ñ–≤ –≥—Ä—É–ø)
            # –ù–∞–ø—Ä–∏–∫–ª–∞–¥, –≥—Ä—É–ø–∏ –Ü–†/–ö–Ü–¢ ‚Üí –§–ö–ù–¢
            if any(x in group_name.upper() for x in ["–Ü–†", "–ö–Ü–¢", "–ö–Ü", "–ü–ú", "–ö–ë"]):
                search_scope = "–§–ö–ù–¢"
                search_level = "faculty"
                confidence = 0.60
        
        # === –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø INTENT ===
        search_intent = self._detect_intent(query_lower)
        
        # === –ì–ï–ù–ï–†–ê–¶–Ü–Ø –ö–õ–Æ–ß–û–í–ò–• –°–õ–Ü–í –î–õ–Ø –ü–û–ö–†–ê–©–ï–ù–ù–Ø ===
        enhancement_keywords = self._generate_enhancement_keywords(query_lower, search_intent)
        
        # === REASONING ===
        reasoning = self._build_reasoning(
            entities, context_scope, context_entity, 
            search_scope, search_level, target_entity
        )
        
        # === –í–ò–ó–ù–ê–ß–ï–ù–ù–Ø needs_database_search ===
        # –ü—Ä–æ—Å—Ç–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è —Ä–µ–∂–∏–º—É fallback
        greeting_words = ["–ø—Ä–∏–≤—ñ—Ç", "–ø—Ä–∏–≤–µ—Ç", "–¥—è–∫—É—é", "—Å–ø–∞—Å–∏–±–æ", "–ø–æ–∫–∞", "–±—É–≤–∞–π", "hi", "hello", "bye"]
        question_words = ["—â–æ", "—è–∫", "–∫–æ–ª–∏", "–¥–µ", "—Ö—Ç–æ", "—á–æ–º—É", "—è–∫–∏–π", "—è–∫–∞", "—è–∫–µ", "—á–∏"]

        query_lower = query.lower()
        is_greeting = any(word in query_lower for word in greeting_words)
        has_question = any(word in query_lower for word in question_words)

        # –Ø–∫—â–æ –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø–∏—Ç —ñ –ø—Ä–∏–≤—ñ—Ç–∞–Ω–Ω—è - –Ω–µ —à—É–∫–∞—î–º–æ
        # –Ø–∫—â–æ —î –∑–∞–ø–∏—Ç–∞–ª—å–Ω—ñ —Å–ª–æ–≤–∞ - —à—É–∫–∞—î–º–æ
        needs_search = (not (len(query.split()) <= 3 and is_greeting)) or has_question

        return RouteDecision(
            search_scope=search_scope,
            search_level=search_level,
            target_entity=target_entity,
            search_intent=search_intent,
            enhancement_keywords=enhancement_keywords,
            confidence=0.5,
            reasoning=reasoning,
            needs_database_search=needs_search
        )
    
    async def _llm_routing(self, query: str, history: Optional[List[Dict]], 
                          group_name: Optional[str], max_retries: int = 3) -> Optional[RouteDecision]:
        """
         LLM –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—è –∑ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–∏–º–∏ retry –ø—Ä–∏ –ø–æ–º–∏–ª–∫–∞—Ö –ø–∞—Ä—Å–∏–Ω–≥—É JSON
        
        Args:
            max_retries: –º–∞–∫—Å–∏–º—É–º —Å–ø—Ä–æ–± (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 3)
        """
        
        system_prompt = self._build_llm_routing_prompt()
        
        history_text = ""
        if history and len(history) > 0:
            history_to_show = history.copy()
            if history_to_show and history_to_show[-1].get('role') == 'user':
                last_content = history_to_show[-1].get('content', '').strip()
                if last_content == query.strip():
                    history_to_show = history_to_show[:-1]            
            if history_to_show:
                history_text = "\n".join([
                    f"{'üë§ –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á' if m['role'] == 'user' else 'ü§ñ –ê—Å–∏—Å—Ç–µ–Ω—Ç'}: {m['content']}" 
                    for m in history_to_show
                ])
        
        user_message = f"""–ö–û–ù–¢–ï–ö–°–¢ –î–Ü–ê–õ–û–ì–£:
    {history_text if history_text else "–ü–æ—á–∞—Ç–æ–∫ –¥—ñ–∞–ª–æ–≥—É"}

    –ì–†–£–ü–ê –°–¢–£–î–ï–ù–¢–ê: {group_name if group_name else "–ù–µ –≤–∫–∞–∑–∞–Ω–æ"}

    –ù–û–í–ò–ô –ó–ê–ü–ò–¢: "{query}"

    –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–π –∑–∞–ø–∏—Ç –∑ —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º –∫–æ–Ω—Ç–µ–∫—Å—Ç—É —Ç–∞ –¥–∞–π —Ä—ñ—à–µ–Ω–Ω—è –ø—Ä–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—é."""

        # RETRY LOOP
        for attempt in range(1, max_retries + 1):
            try:
                logger.debug(f"ü§ñ LLM ROUTING: —Å–ø—Ä–æ–±–∞ {attempt}/{max_retries}")
                
                # ========== –ó–ê–ú–Ü–ù–ê –ù–ê GEMINI ==========
                if settings.USE_GEMINI:
                    import google.generativeai as genai
                    
                    full_prompt = f"{system_prompt}\n\n{user_message}"
                    
                    response = self.gemini_model.generate_content(
                        full_prompt,
                        generation_config=genai.types.GenerationConfig(
                            temperature=0.3,
                            max_output_tokens=1000,
                        )
                    )
                    
                    llm_response = response.text
                    logger.debug(f"ü§ñ LLM –í–Ü–î–ü–û–í–Ü–í: {llm_response}")
                    
                    # –ü–ê–†–°–ò–ù–ì JSON
                    parsed = self._extract_json_from_llm(llm_response)
                    
                    if parsed:
                        logger.debug(f"‚úÖ JSON —É—Å–ø—ñ—à–Ω–æ —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏–π –Ω–∞ —Å–ø—Ä–æ–±—ñ {attempt}")
                        
                        return RouteDecision(
                            search_scope=parsed.get("search_scope", "global"),
                            search_level=parsed.get("search_level", "general"),
                            target_entity=parsed.get("target_entity"),
                            search_intent=parsed.get("search_intent", "info"),
                            enhancement_keywords=parsed.get("enhancement_keywords", []),
                            confidence=parsed.get("confidence", 0.5),
                            reasoning=parsed.get("reasoning", "LLM routing"),
                            needs_database_search=parsed.get("needs_database_search", True)
                        )
                    else:
                        logger.warning(f"‚ö†Ô∏è LLM routing (—Å–ø—Ä–æ–±–∞ {attempt}): –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON")
                        if attempt < max_retries:
                            logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞...")
                            continue
                        else:
                            logger.error(f"‚ùå –í–∏—á–µ—Ä–ø–∞–Ω–æ —Å–ø—Ä–æ–±–∏ ({max_retries}), fallback –Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫—É")
                            return None
                
                # ========== –°–¢–ê–†–ò–ô –ö–û–î –î–õ–Ø LM STUDIO ==========
                else:
                    async with httpx.AsyncClient(timeout=30.0) as client:
                        response = await client.post(
                            self.lm_studio_url,
                            json={
                                "messages": [
                                    {"role": "system", "content": system_prompt},
                                    {"role": "user", "content": user_message}
                                ],
                                "temperature": 0.3,
                                "max_tokens": 1000
                            }
                        )
                        
                        if response.status_code == 200:
                            data = response.json()
                            llm_response = data["choices"][0]["message"]["content"]
                            logger.debug(f"ü§ñ LLM –í–Ü–î–ü–û–í–Ü–í: {llm_response[:200]}...")
                            
                            # –ü–ê–†–°–ò–ù–ì JSON
                            parsed = self._extract_json_from_llm(llm_response)
                            
                            if parsed:
                                logger.debug(f"‚úÖ JSON —É—Å–ø—ñ—à–Ω–æ —Ä–æ–∑–ø–∞—Ä—Å–µ–Ω–∏–π –Ω–∞ —Å–ø—Ä–æ–±—ñ {attempt}")
                                
                                return RouteDecision(
                                    search_scope=parsed.get("search_scope", "global"),
                                    search_level=parsed.get("search_level", "general"),
                                    target_entity=parsed.get("target_entity"),
                                    search_intent=parsed.get("search_intent", "info"),
                                    enhancement_keywords=parsed.get("enhancement_keywords", []),
                                    confidence=parsed.get("confidence", 0.5),
                                    reasoning=parsed.get("reasoning", "LLM routing"),
                                    needs_database_search=parsed.get("needs_database_search", True)
                                )
                            else:
                                logger.warning(f"‚ö†Ô∏è LLM routing (—Å–ø—Ä–æ–±–∞ {attempt}): –Ω–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø–∞—Ä—Å–∏—Ç–∏ JSON")
                                if attempt < max_retries:
                                    logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞...")
                                    continue  
                                else:
                                    logger.error(f"‚ùå –í–∏—á–µ—Ä–ø–∞–Ω–æ —Å–ø—Ä–æ–±–∏ ({max_retries}), fallback –Ω–∞ –µ–≤—Ä–∏—Å—Ç–∏–∫—É")
                                    return None
                        else:
                            logger.warning(f"‚ö†Ô∏è LLM routing (—Å–ø—Ä–æ–±–∞ {attempt}): HTTP {response.status_code}")
                            if attempt < max_retries:
                                continue
                            return None
                            
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è LLM routing (—Å–ø—Ä–æ–±–∞ {attempt}): –ø–æ–º–∏–ª–∫–∞ {e}")
                if attempt < max_retries:
                    logger.debug(f"üîÑ –ü–æ–≤—Ç–æ—Ä–Ω–∞ —Å–ø—Ä–æ–±–∞...")
                    continue
                return None
        
        # –Ø–∫—â–æ –≤—Å—ñ —Å–ø—Ä–æ–±–∏ –ø—Ä–æ–≤–∞–ª–∏–ª–∏—Å—è
        return None
    
    def _build_llm_routing_prompt(self) -> str:
        """–ü–æ–±—É–¥–æ–≤–∞ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è LLM"""
        
        # –ì–µ–Ω–µ—Ä—É—î–º–æ –∫–æ–º–ø–∞–∫—Ç–Ω–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ –ö–ê–Ü
        structure_summary = []
        for faculty_code, faculty_data in self.nau_structure.items():
            if faculty_code == "global":
                continue
            structure_summary.append(f"- {faculty_code}: {faculty_data['full_name']}")
            for dept_code, dept_data in faculty_data.get("departments", {}).items():
                structure_summary.append(f"  - {dept_code}: {dept_data['full_name']}")
        
        structure_text = "\n".join(structure_summary)
        
        return f"""You are an expert routing system for National Aviation University (NAU).

NAU STRUCTURE:
{structure_text}

‚îÅ‚îÅ‚îÅ‚îÅ
üß† CRITICAL: DIALOGUE CONTEXT HANDLING
‚îÅ‚îÅ‚îÅ‚îÅ

üìú WHAT IS CONTEXT:
- You ALWAYS receive FULL dialogue history with the user
- Each previous message is a KEY to understanding the new query
- User does NOT repeat information - expects you to remember the dialogue

üéØ MAIN CONTEXT RULE:
If user does NOT specify who/what they're asking about in new query ‚Üí they MEAN 
the same thing discussed in PREVIOUS MESSAGES!

üîç HOW TO ANALYZE CONTEXT:

1Ô∏è ALWAYS read ENTIRE dialogue history BEFORE analyzing new query
2Ô∏è Look for key entities in previous messages:
   - People's names (teachers, students, staff)
   - Names (departments, faculties, events, conferences)
   - Conversation topics (sports, science, education)
   
3Ô∏è If new query contains pronouns or vague expressions:
   - "he", "she", "they" ‚Üí WHO was mentioned before?
   - "this", "that" ‚Üí WHAT was discussed before?
   - "there" ‚Üí WHERE was the conversation about?
   - Just "teacher" WITHOUT name ‚Üí WHICH teacher was discussed?

4Ô∏è MANDATORY add to enhancement_keywords:
   - ALL names and surnames from context, if query is about THAT SAME person
   - ALL event/location names from context, if query is about them
   - Synonyms and variations of what was discussed

5Ô∏è IF new query topic is UNRELATED to previous context, DON'T reference it

‚îÅ‚îÅ‚îÅ‚îÅ
üìö CONTEXT HANDLING EXAMPLES:
‚îÅ‚îÅ‚îÅ‚îÅ

EXAMPLE 1 - Person clarification:
Context:
  üë§ User: –•—Ç–æ –∑–∞–≤—ñ–¥—É–≤–∞—á –Ü–ü–ó?
  ü§ñ Assistant: –ó–∞–≤—ñ–¥—É–≤–∞—á –∫–∞—Ñ–µ–¥—Ä–∏ –Ü–ü–ó - –ø—Ä–æ—Ñ–µ—Å–æ—Ä –Ü–≤–∞–Ω–æ–≤ –ü–µ—Ç—Ä–æ –°—Ç–µ–ø–∞–Ω–æ–≤–∏—á
New query: "–ê –π–æ–≥–æ email?"

YOUR ANALYSIS:
{{
  "reasoning": "Reading dialogue history... See that Ivanov Petro Stepanovych, head of IPZ, was discussed earlier. New query 'his email' - pronoun 'his' refers to this person from context. This is continuation of topic about IPZ teacher. Setting: scope=–§–ö–ù–¢ (because IPZ is there), entity=–Ü–ü–ó, intent=contacts (asking for email). Adding to keywords full name + position synonyms.",
  "search_scope": "–§–ö–ù–¢",
  "search_level": "department",
  "target_entity": "–Ü–ü–ó",
  "search_intent": "contacts",
  "enhancement_keywords": ["–Ü–≤–∞–Ω–æ–≤", "–ü–µ—Ç—Ä–æ", "–°—Ç–µ–ø–∞–Ω–æ–≤–∏—á", "–≤–∏–∫–ª–∞–¥–∞—á", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "–∑–∞–≤—ñ–¥—É–≤–∞—á", "email", "–∫–æ–Ω—Ç–∞–∫—Ç–∏"],
  "confidence": 0.95,
  "needs_database_search": true
}}

‚îÅ‚îÅ‚îÅ‚îÅ

EXAMPLE 2 - Topic continuation without clarification:
Context:
  üë§ User: –ö—Ç–æ —Ç–∞–∫–æ–π –ú–∞–ª—è—Ä—á—É–∫?
  ü§ñ Assistant: [explanation that it's a joke about teachers]
  üë§ User: –ê –Ω–∞ –§–ö–ù–¢ —î —Ç–∞–∫—ñ?
  ü§ñ Assistant: [list of FKNT teachers]
New query: "—Ç–∞ —Ü–µ –∑ –∫—Å–º –ø—Ä–µ–ø–æ–¥"

YOUR ANALYSIS:
{{
  "reasoning": "Analyzing context... Entire dialogue is about NAU teachers. First asked about Malyarchuk (joke), then about FKNT teachers. New query 'this is from ksm teacher' - 'this' = teacher (dialogue topic), 'ksm' = KSM department, 'teacher' = teacher. User clarifies it's specifically about KSM teacher. Setting scope=–§–ö–ù–¢, entity=–ö–°–ú. Adding 'Malyarchuk' to keywords as initial dialogue topic.",
  "search_scope": "–§–ö–ù–¢",
  "search_level": "department",
  "target_entity": "–ö–°–ú",
  "search_intent": "info",
  "enhancement_keywords": ["–≤–∏–∫–ª–∞–¥–∞—á", "–≤–∏–∫–ª–∞–¥–∞—á—ñ", "–ø—Ä–æ—Ñ–µ—Å–æ—Ä", "–¥–æ—Ü–µ–Ω—Ç", "–ø–µ—Ä—Å–æ–Ω–∞–ª", "–ö–°–ú", "–ú–∞–ª—è—Ä—á—É–∫"],
  "confidence": 0.85,
  "needs_database_search": true
}}

‚îÅ‚îÅ‚îÅ‚îÅ
‚öôÔ∏è ROUTING PARAMETERS:
‚îÅ‚îÅ‚îÅ‚îÅ

TASK: Determine the following parameters:

1. reasoning: detailed explanation of your analysis (FIRST FIELD!)
   - How you used dialogue history
   - What pronouns/vague expressions mean from context
   - Why you chose such scope/entity/intent
   - Which keywords you added and why

2. search_scope: "–§–ö–ù–¢" / "–§–ê–ï–¢" / "global"
   - Faculty if specific department mentioned
   - "global" if general question about NAU

3. search_level: "faculty" / "department" / "general"  
   - "department" if about specific department
   - "faculty" if about faculty in general
   - "general" for general questions

4. target_entity: department code (–Ü–ü–ó/–ö–Ü–¢/–ö–°–ú/...) or null
   - Fill if department exists in context or query

5. search_intent: "info" / "schedule" / "news" / "contacts" / "events"
   - "info" - general information
   - "contacts" - email, phones, communication
   - "news" - news, announcements
   - "events" - events, conferences, activities

6. enhancement_keywords: list of search keywords (SEE DETAILED RULES BELOW!)

7. confidence: 0.0-1.0
   - High (0.9+) if everything is clear from context
   - Medium (0.7-0.9) if there are assumptions
   - Low (<0.7) if much uncertainty

8. needs_database_search: true / false

‚îÅ‚îÅ‚îÅ‚îÅ
‚ö†Ô∏è CRITICAL: ENHANCEMENT_KEYWORDS RULES
‚îÅ‚îÅ‚îÅ‚îÅ

**MAIN RULE:** Generate keywords that **EXIST IN NEWS**, not in questions!

**HOW EMBEDDING SEARCH WORKS:**
- Searches for **vector similarity** between query and news text
- If query has words that are **ABSENT** in news ‚Üí search fails
- News contains: event names, names, places, topics
- News does NOT contain: question words, temporal markers from questions

‚úÖ **CORRECT KEYWORDS** (exist in news):
- Topics: "—Ç–µ–Ω—ñ—Å", "—Ñ—É—Ç–±–æ–ª", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è", "—Å–µ–º—ñ–Ω–∞—Ä"
- Names: "–Ü–≤–∞–Ω–æ–≤", "–ú–∞–ª—è—Ä—á—É–∫", "–¢—É—Ä—É–π"  
- Places: "–ù–ê–£", "–§–ö–ù–¢", "–ö–°–ú", "—Å–ø–æ—Ä—Ç–∫–æ–º–ø–ª–µ–∫—Å"
- Events: "–£–Ω—ñ–≤–µ—Ä—Å—ñ–∞–¥–∞", "—Ç—É—Ä–Ω—ñ—Ä", "–∑–º–∞–≥–∞–Ω–Ω—è"

‚ùå **WRONG KEYWORDS** (absent in news):
- Question words: "–∫–æ–ª–∏", "–¥–µ", "—â–æ", "—Ö—Ç–æ", "—è–∫"
- Verbs from questions: "–±—É–≤", "–±—É–¥–µ", "–≤—ñ–¥–±—É–≤—Å—è"
- Temporal markers FROM QUESTIONS: "–æ—Å—Ç–∞–Ω–Ω—è", "–Ω–∞—Å—Ç—É–ø–Ω–∞", "–≤—á–æ—Ä–∞", "–∑–∞–≤—Ç—Ä–∞"

‚ö†Ô∏è **NOTE:** Temporal markers in questions ("–æ—Å—Ç–∞–Ω–Ω—è –≥—Ä–∞", "–æ—Å—Ç–∞–Ω–Ω—ñ–π —Ç—É—Ä–Ω—ñ—Ä") should be REMOVED. News will be found by topic words.

üß† **KEYWORD GENERATION ALGORITHM:**
1. Extract NOUNS from query (people, places, events, topics)
2. Add SYNONYMS of these nouns
3. Add CONTEXT (faculty, department, event type)
4. REMOVE everything else: question words, verbs, temporal markers, service words

**EXAMPLES:**
Query: "–∫–æ–ª–∏ –±—É–ª–∞ –æ—Å—Ç–∞–Ω–Ω—è –≥—Ä–∞ –≤ —Ç–µ–Ω—ñ—Å?"
‚ùå BAD: ["–∫–æ–ª–∏", "–±—É–ª–∞", "–æ—Å—Ç–∞–Ω–Ω—è", "–≥—Ä–∞", "—Ç–µ–Ω—ñ—Å"]
‚úÖ GOOD: ["—Ç–µ–Ω—ñ—Å", "–≥—Ä–∞", "—Å–ø–æ—Ä—Ç", "–∑–º–∞–≥–∞–Ω–Ω—è", "–ù–ê–£"]

Query: "—Ö—Ç–æ –≤–∏–≥—Ä–∞–≤ –æ—Å—Ç–∞–Ω–Ω—ñ–π —Ç—É—Ä–Ω—ñ—Ä?"
‚ùå BAD: ["—Ö—Ç–æ", "–≤–∏–≥—Ä–∞–≤", "–æ—Å—Ç–∞–Ω–Ω—ñ–π", "—Ç—É—Ä–Ω—ñ—Ä"]  
‚úÖ GOOD: ["—Ç—É—Ä–Ω—ñ—Ä", "–ø–µ—Ä–µ–º–æ–∂–µ—Ü—å", "–∑–º–∞–≥–∞–Ω–Ω—è", "—Å–ø–æ—Ä—Ç", "–ù–ê–£"]

‚îÅ‚îÅ‚îÅ‚îÅ
‚úÖ RULES FOR needs_database_search:
‚îÅ‚îÅ‚îÅ‚îÅ

üü¢ TRUE (search in DB):
- Factual questions about NAU (teachers, events, news)
- Clarifications about something mentioned in context
- Questions about sports, science, NAU conferences
- General "who", "what", "when", "where" questions about NAU

üî¥ FALSE (DON'T search):
- Greetings ("–ø—Ä–∏–≤—ñ—Ç", "–¥—è–∫—É—é")
- Requests to explain ALREADY PROVIDED information
- Philosophical questions not about NAU
- ‚ö†Ô∏è Schedule queries ("—Ä–æ–∑–∫–ª–∞–¥", "—è–∫—ñ –ø–∞—Ä–∏")

‚îÅ‚îÅ‚îÅ‚îÅ
üì§ RESPONSE FORMAT:
‚îÅ‚îÅ‚îÅ‚îÅ

‚ö†Ô∏è CRITICAL: MANDATORY FIELD ORDER!

**reasoning ALWAYS FIRST FIELD!** Think first, then decide.

RESPOND ONLY WITH JSON, NO ADDITIONAL TEXT:

{{
  "reasoning": "First analyzing dialogue context... [here you describe in detail your thinking process: what you see in history, how you interpret new query, why you choose these parameters]",
  "search_scope": "–§–ö–ù–¢",
  "search_level": "department",
  "target_entity": "–ö–°–ú",
  "search_intent": "info",
  "enhancement_keywords": ["–≤–∏–∫–ª–∞–¥–∞—á", "–ø—Ä–æ—Ñ–µ—Å–æ—Ä", "–¥–æ—Ü–µ–Ω—Ç", "–ö–°–ú", "–ø–µ—Ä—Å–æ–Ω–∞–ª"],
  "confidence": 0.85,
  "needs_database_search": true
}}

**HOW TO WRITE REASONING:**
‚úÖ GOOD: "Reading dialogue history... See that Malyarchuk from KSM was discussed earlier. New query 'his number' - pronoun 'his' = Malyarchuk. This continues teacher topic. Setting scope=–§–ö–ù–¢ (KSM belongs to FKNT), entity=–ö–°–ú, intent=contacts (number = contact). Adding to keywords 'Malyarchuk' + teacher synonyms + '–∫–æ–Ω—Ç–∞–∫—Ç–∏ —Ç–µ–ª–µ—Ñ–æ–Ω'."

‚ùå BAD: "Query about teacher" (too short, no context analysis)

GENERATE KEYWORD QUERIES THAT COULD BE USED ON A WEBSITE RELATED TO THE TOPIC OF THE NEWS AND CONNECTED TO ITS TITLE."""
    
    def _detect_intent(self, query: str) -> str:
        """–í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞–º—ñ—Ä—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"""
        
        intent_patterns = {
            "schedule": ["—Ä–æ–∑–∫–ª–∞–¥", "–ø–∞—Ä–∏", "–∑–∞–Ω—è—Ç—Ç—è", "–∫–æ–ª–∏", "–æ –∫–æ—Ç—Ä—ñ–π"],
            "news": ["–Ω–æ–≤–∏–Ω", "–ø–æ–¥—ñ–π", "–æ—Å—Ç–∞–Ω–Ω", "—â–æ –Ω–æ–≤–æ–≥–æ", "–∞–∫—Ç—É–∞–ª—å–Ω"],
            "contacts": ["–∫–æ–Ω—Ç–∞–∫—Ç", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–∞–¥—Ä–µ—Å", "email", "–¥–µ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è"],
            "events": ["–∑–∞—Ö—ñ–¥", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü—ñ—è", "—Å–µ–º—ñ–Ω–∞—Ä", "–∑—É—Å—Ç—Ä—ñ—á", "—Ñ–æ—Ä—É–º", "–ø–æ–ª—ñ—Ç"],
            "info": ["—ñ–Ω—Ñ–æ—Ä–º–∞—Ü", "—Ä–æ–∑–∫–∞–∂–∏", "—â–æ", "—è–∫", "—Ö—Ç–æ", "–≤–∏–∫–ª–∞–¥–∞—á"],
        }
        
        for intent, patterns in intent_patterns.items():
            if any(pattern in query for pattern in patterns):
                return intent
        
        return "info"
    
    def _generate_enhancement_keywords(self, query: str, intent: str) -> List[str]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –¥–æ–¥–∞—Ç–∫–æ–≤–∏—Ö –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤ –¥–ª—è –ø–æ–ª—ñ–ø—à–µ–Ω–Ω—è –ø–æ—à—É–∫—É
        """
        
        keywords = []
        
        # 1. –°–º–∏—Å–ª–æ–≤—ñ —Ä–æ–∑—à–∏—Ä–µ–Ω–Ω—è –∑—ñ —Å–ª–æ–≤–Ω–∏–∫–∞
        for key, expansions in self.semantic_expansions.items():
            if key in query:
                keywords.extend(expansions[:3])  # –¢–æ–ø-3
        
        # 2. Intent-based keywords
        intent_keywords = {
            "schedule": ["–≥—Ä–∞—Ñ—ñ–∫", "—á–∞—Å", "–∞—É–¥–∏—Ç–æ—Ä—ñ—è"],
            "news": ["–ø–æ–¥—ñ—è", "–æ–≥–æ–ª–æ—à–µ–Ω–Ω—è", "—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è"],
            "contacts": ["–∑–≤'—è–∑–æ–∫", "—Ç–µ–ª–µ—Ñ–æ–Ω", "–∞–¥—Ä–µ—Å–∞"],
            "events": ["–ø–æ–¥—ñ—è", "–∑—É—Å—Ç—Ä—ñ—á", "–∑–∞—Ö—ñ–¥"],
            "info": ["—ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è", "–¥–∞–Ω—ñ", "–≤—ñ–¥–æ–º–æ—Å—Ç—ñ"],
        }
        
        keywords.extend(intent_keywords.get(intent, []))
        
        # 3. –ü—Ä–∏–±–∏—Ä–∞—î–º–æ –¥—É–±–ª—ñ–∫–∞—Ç–∏
        return list(set(keywords))[:5]  # –ú–∞–∫—Å–∏–º—É–º 5
    
    def _build_reasoning(self, entities, context_scope, context_entity,
                        search_scope, search_level, target_entity) -> str:
        """–ü–æ–±—É–¥–æ–≤–∞ –ø–æ—è—Å–Ω–µ–Ω–Ω—è —Ä—ñ—à–µ–Ω–Ω—è"""
        
        if entities:
            entity = entities[0]
            return f"–ó–Ω–∞–π–¥–µ–Ω–æ: {entity['matched_alias']} ‚Üí {entity['full_name']}"
        elif context_entity:
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∑ —ñ—Å—Ç–æ—Ä—ñ—ó: {context_entity}"
        elif context_scope:
            return f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –∑ —ñ—Å—Ç–æ—Ä—ñ—ó: —Ñ–∞–∫—É–ª—å—Ç–µ—Ç {context_scope}"
        elif search_scope != "global":
            return f"–í–∏–∑–Ω–∞—á–µ–Ω–æ —Ñ–∞–∫—É–ª—å—Ç–µ—Ç: {search_scope}"
        else:
            return "–ó–∞–≥–∞–ª—å–Ω–∏–π –∑–∞–ø–∏—Ç –ø—Ä–æ –ù–ê–£"
    
    def _extract_json_from_llm(self, text: str) -> Optional[Dict]:
        """–í–∏—Ç—è–≥ JSON –∑ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ LLM"""
        
        # –ò—â–µ–º JSON –±–ª–æ–∫
        json_match = re.search(r'\{[^\}]+\}', text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass
        
        return None


# === –î–û–ü–û–ú–Ü–ñ–ù–Ü –§–£–ù–ö–¶–Ü–á ===

def format_route_for_search(route: RouteDecision) -> Dict:
    """
    –§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è RouteDecision –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –≤ database.py
    
    –ü–æ–≤–µ—Ä—Ç–∞—î:
        Dict –∑ —Ñ—ñ–ª—å—Ç—Ä–∞–º–∏ –¥–ª—è ChromaDB
    """
    
    filters = {}
    
    # Scope —Ñ—ñ–ª—å—Ç—Ä
    if route.search_scope != "global":
        filters["faculty"] = route.search_scope
    
    # Entity —Ñ—ñ–ª—å—Ç—Ä
    if route.target_entity:
        filters["department"] = route.target_entity
    
    # Intent —Ñ—ñ–ª—å—Ç—Ä (–∫–∞—Ç–µ–≥–æ—Ä—ñ—è)
    intent_to_category = {
        "schedule": "schedule",
        "news": "news",
        "events": "events",
        "contacts": "contacts",
        "info": None
    }
    
    category = intent_to_category.get(route.search_intent)
    if category:
        filters["category"] = category
    
    return filters


def enhance_query_with_route(query: str, route: RouteDecision) -> str:
    """
    –†–æ–∑—à–∏—Ä–µ–Ω–Ω—è –∑–∞–ø–∏—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –∑ –º–∞—Ä—à—Ä—É—Ç—É
    
    Args:
        query: –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –∑–∞–ø–∏—Ç
        route: –†—ñ—à–µ–Ω–Ω—è –ø—Ä–æ –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü—ñ—é
    
    Returns:
        –†–æ–∑—à–∏—Ä–µ–Ω–∏–π –∑–∞–ø–∏—Ç
    """
    
    parts = [query]
    
    # –î–æ–¥–∞—î–º–æ –ø–æ–≤–Ω—ñ –Ω–∞–∑–≤–∏
    if route.search_scope != "global" and route.search_scope in NAU_STRUCTURE:
        faculty = NAU_STRUCTURE[route.search_scope]
        parts.append(faculty["full_name"])
    
    if route.target_entity and route.search_scope in NAU_STRUCTURE:
        dept = NAU_STRUCTURE[route.search_scope]["departments"].get(route.target_entity)
        if dept:
            parts.append(dept["full_name"])
    
    # –î–æ–¥–∞—î–º–æ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∫—Ä–∞—â–µ–Ω–Ω—è
    if route.enhancement_keywords:
        parts.extend(route.enhancement_keywords)
    
    return " ".join(parts)


# –≠–∫—Å–ø–æ—Ä—Ç
__all__ = [
    'QueryRouter',
    'RouteDecision',
    'format_route_for_search',
    'enhance_query_with_route'
]